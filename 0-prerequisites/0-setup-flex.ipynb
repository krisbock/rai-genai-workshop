{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating with the Azure AI Evaluation SDK\n",
    "\n",
    "https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/flow-evaluate-sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Azure OpenAI resource created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate with AI-assisted metrics, you need:\n",
    "\n",
    "A test dataset in .jsonl format. See the next section for dataset requirements\n",
    "A deployment of one of these models: GPT 3.5 models, GPT 4 models, or Davinci models AND an embedding model for grounded responses with RAG.\n",
    "Ideally, GPT 4 models are recommended for the best evaluation capabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add steps to create an Azure OpenAI resource and deploy a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install into current notebook environment\n",
    "import sys\n",
    "!{sys.executable} -m pip install azure-ai-evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare config files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create .env file containing secrets\n",
    "```\n",
    "SUBSCRIPTION_ID=\n",
    "RESOURCE_GROUP_NAME=\n",
    "PROJECT_NAME=\n",
    "AZURE_OPENAI_ENDPOINT=\n",
    "AZURE_OPENAI_EVALUATION_DEPLOYMENT=\n",
    "# Uncomment if using key-based auth\n",
    "# AZURE_OPENAI_KEY=\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env', override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise Azure OpenAI connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make sure user is Azure OpenAI contributor\n",
    "https://learn.microsoft.com/en-us/azure/ai-studio/concepts/rbac-ai-studio#scenario-use-an-existing-azure-openai-resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Initialize Azure OpenAI Connection with your environment variables\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    \"api_version\": os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from promptflow.core import AzureOpenAIModelConfiguration\n",
    "\n",
    "# Initialize Azure OpenAI Connection with your environment variables\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_deployment=os.environ.get(\"AZURE_OPENAI_EVALUATION_DEPLOYMENT\"),\n",
    "    api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    ")\n",
    "\n",
    "# #Initialize Azure OpenAI Connection with your environment variables\n",
    "# model_config = AzureOpenAIModelConfiguration(\n",
    "#     azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "#     azure_deployment=os.environ.get(\"AZURE_OPENAI_EVALUATION_DEPLOYMENT\"),\n",
    "#     api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "# )\n",
    "\n",
    "# Initialize Azure OpenAI Connection using AI Studio connection\n",
    "# from promptflow.rag.config import ConnectionConfig\n",
    "\n",
    "# model_connect_config = ConnectionConfig(\n",
    "#     subscription_id = os.environ.get(\"SUBSCRIPTION_ID\"),\n",
    "#     resource_group_name = os.environ.get(\"RESOURCE_GROUP_NAME\"),\n",
    "#     workspace_name = os.environ.get(\"PROJECT_NAME\"),\n",
    "#     connection_name = \"mssecureai4034688619\"\n",
    "# )\n",
    "# model_connect = AzureOpenAIConnection(\n",
    "#     api_base = os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "#     auth_mode = \"meid_token\"\n",
    "# )\n",
    "model_connect = AzureOpenAIConnection(\n",
    "    name=os.environ.get(\"AISTUDIO_AOAI_CONNECTION_NAME\"),\n",
    "    api_base=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_type=\"azure\",\n",
    "    api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    ")\n",
    "\n",
    "# model_config = AzureOpenAIModelConfiguration(\n",
    "#     azure_deployment=\"gpt-4o\",\n",
    "#     connection=model_connect\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # try using connection config\n",
    "# # from promptflow.rag.config import ConnectionConfig\n",
    "\n",
    "# model_connect_config = ConnectionConfig(\n",
    "#     subscription_id = os.environ.get(\"SUBSCRIPTION_ID\"),\n",
    "#     resource_group_name = os.environ.get(\"RESOURCE_GROUP_NAME\"),\n",
    "#     workspace_name = os.environ.get(\"PROJECT_NAME\"),\n",
    "#     connection_name = \"mssecureai4034688619\"\n",
    "\n",
    "# model_connect = AzureOpenAIModelConfiguration.from_connection(model_connect_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gpt_relevance': 5.0}\n"
     ]
    }
   ],
   "source": [
    "from promptflow.evals.evaluators import RelevanceEvaluator\n",
    "\n",
    "# Initialzing Relevance Evaluator\n",
    "relevance_eval = RelevanceEvaluator(model_config)\n",
    "# Running Relevance Evaluator on single input row\n",
    "relevance_score = relevance_eval(\n",
    "    answer=\"The Alpine Explorer Tent is the most waterproof.\",\n",
    "    context=\"From the our product list,\"\n",
    "    \" the alpine explorer tent is the most waterproof.\"\n",
    "    \" The Adventure Dining Table has higher weight.\",\n",
    "    question=\"Which tent is the most waterproof?\",\n",
    ")\n",
    "print(relevance_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Risk and Safety Evaluators\n",
    "GPT not required - instead we use Azure AI Studio safety evaluations back-end service.\n",
    "\n",
    "Note - Risk and safety metrics are only available in the following regions: East US 2, France Central, UK South, Sweden Central. \n",
    "\n",
    "***Groundedness measurement leveraging Azure AI Content Safety Groundedness Detection is only supported following regions: East US 2 and Sweden Central.***\n",
    "\n",
    "Check [region-availability](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/overview#region-availability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "# token_provider = get_bearer_token_provider(\n",
    "#     DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the Azure AI Studio connection\n",
    "azure_ai_project = {\n",
    "    \"subscription_id\": os.environ.get(\"SUBSCRIPTION_ID\"),\n",
    "    \"resource_group_name\": os.environ.get(\"RESOURCE_GROUP_NAME\"),\n",
    "    \"project_name\": os.environ.get(\"PROJECT_NAME\"),\n",
    "    #\"credential\": token_provider,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-05 21:14:59 +1000][flowinvoker][INFO] - Getting connections from pf client with provider from args: local...\n",
      "[2024-08-05 21:14:59 +1000][flowinvoker][INFO] - Promptflow get connections successfully. keys: dict_keys([])\n",
      "[2024-08-05 21:14:59 +1000][flowinvoker][INFO] - Promptflow executor starts initializing...\n",
      "[2024-08-05 21:14:59 +1000][flowinvoker][INFO] - Promptflow executor initiated successfully.\n",
      "[2024-08-05 21:14:59 +1000][flowinvoker][INFO] - Validating flow input with data {'metric_name': 'violence', 'question': 'What is the capital of France?', 'answer': 'Paris.', 'project_scope': {'subscription_id': '3c8972d9-f541-46b2-b70b-d81baba3595d', 'resource_group_name': 'secure-ai-rg', 'project_name': 'krbock-0635'}, 'credential': None}\n",
      "[2024-08-05 21:14:59 +1000][flowinvoker][INFO] - Execute flow with data {'metric_name': 'violence', 'question': 'What is the capital of France?', 'answer': 'Paris.', 'project_scope': {'subscription_id': '3c8972d9-f541-46b2-b70b-d81baba3595d', 'resource_group_name': 'secure-ai-rg', 'project_name': 'krbock-0635'}, 'credential': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-05 21:14:59 +1000     816 execution.flow     INFO     Start executing nodes in thread pool mode.\n",
      "2024-08-05 21:14:59 +1000     816 execution.flow     INFO     Start to run 2 nodes with concurrency level 16.\n",
      "2024-08-05 21:14:59 +1000     816 execution.flow     INFO     Executing node validate_inputs. node run id: d1f52e2a-beaf-4524-a04b-2c0b71361418_validate_inputs_67a661ef-f66d-4205-95c3-0b75dc58f374\n",
      "2024-08-05 21:14:59 +1000     816 execution.flow     INFO     Node validate_inputs completes.\n",
      "2024-08-05 21:14:59 +1000     816 execution.flow     INFO     The node 'evaluate_with_rai_service' will be executed because the activate condition is met, i.e. '${validate_inputs.output}' is equal to 'True'.\n",
      "2024-08-05 21:14:59 +1000     816 execution.flow     INFO     Executing node evaluate_with_rai_service. node run id: d1f52e2a-beaf-4524-a04b-2c0b71361418_evaluate_with_rai_service_93a9082c-83fc-40f3-bd36-e5f721e3cb9e\n",
      "2024-08-05 21:15:09 +1000     816 execution.flow     INFO     Node evaluate_with_rai_service completes.\n",
      "{'violence': 'Very low', 'violence_score': 0, 'violence_reason': \"The system's response provides factual information without any violent content or implications.\"}\n"
     ]
    }
   ],
   "source": [
    "from promptflow.evals.evaluators import ViolenceEvaluator\n",
    "\n",
    "# Initialzing Violence Evaluator with project information\n",
    "violence_eval = ViolenceEvaluator(azure_ai_project)\n",
    "# Running Violence Evaluator on single input row\n",
    "violence_score = violence_eval(question=\"What is the capital of France?\", answer=\"Paris.\")\n",
    "print(violence_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine local dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'Which tent is the most waterproof?',\n",
       "  'truth': 'The Alpine Explorer Tent has the highest rainfly waterproof rating at 3000m'},\n",
       " {'question': 'Which camping table holds the most weight?',\n",
       "  'truth': 'The Adventure Dining Table has a higher weight capacity than all of the other camping tables mentioned'},\n",
       " {'question': 'How much does TrailWalker Hiking Shoes cost? ',\n",
       "  'truth': '$110'},\n",
       " {'question': 'What is the proper care for trailwalker hiking shoes? ',\n",
       "  'truth': 'After each use, remove any dirt or debris by brushing or wiping the shoes with a damp cloth.'},\n",
       " {'question': 'What brand is for TrailMaster tent? ',\n",
       "  'truth': 'OutdoorLiving'},\n",
       " {'question': 'How do I carry the TrailMaster tent around? ',\n",
       "  'truth': ' Carry bag included for convenient storage and transportation'},\n",
       " {'question': 'What is the floor area for Floor Area? ',\n",
       "  'truth': '80 square feet'},\n",
       " {'question': 'What is the material for TrailBlaze Hiking Pants',\n",
       "  'truth': 'Made of high-quality nylon fabric'},\n",
       " {'question': 'What color does TrailBlaze Hiking Pants come in',\n",
       "  'truth': 'Khaki'},\n",
       " {'question': 'Cant he warrenty for TrailBlaze pants be transfered? ',\n",
       "  'truth': 'he warranty is non-transferable and applies only to the original purchaser of the TrailBlaze Hiking Pants. It is valid only when the product is purchased from an authorized retailer.'}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "def load_jsonl(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return [json.loads(line) for line in f.readlines()]\n",
    "    \n",
    "mydata = load_jsonl('../data/evaluation_dataset.jsonl')\n",
    "mydata[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# create directory for output\n",
    "output_dir = '../data/evaluate'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callable function that invokes Azure OpenAI.  For use as target in evaluator.\n",
    "from genai.llm import llm_tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Evaluator | question | answer | context | ground_truth |\n",
    "| --- | --- | --- | --- | --- | \n",
    "| GroundednessEvaluator | N/A | Required: String | Required: String | N/A |\n",
    "| RelevanceEvaluator | Required: String | Required: String | Required: String | N/A |\n",
    "| CoherenceEvaluator | Required: String | Required: String | N/A | N/A |\n",
    "| FluencyEvaluator | Required: String | Required: String | N/A | N/A |\n",
    "| SimilarityEvaluator | Required: String | Required: String | N/A | Required: String |\n",
    "| F1ScoreEvaluator | N/A | Required: String | N/A | Required: String |\n",
    "| ViolenceEvaluator | Required: String | Required: String | N/A | N/A |\n",
    "| SexualEvaluator | Required: String | Required: String | N/A | N/A |\n",
    "| SelfHarmEvaluator | Required: String | Required: String | N/A | N/A |\n",
    "| HateUnfairnessEvaluator | Required: String | Required: String | N/A | N/A |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a  qa evaluation against the AI studio to ensure the connection is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.evals.evaluators import CoherenceEvaluator, RelevanceEvaluator, GroundednessEvaluator, FluencyEvaluator, SimilarityEvaluator, F1ScoreEvaluator\n",
    "\n",
    "coherence_eval = CoherenceEvaluator(model_config=model_config)\n",
    "relevance_eval = RelevanceEvaluator(model_config=model_config)\n",
    "groundedness_eval = GroundednessEvaluator(model_config=model_config)\n",
    "fluency_eval = FluencyEvaluator(model_config=model_config)\n",
    "similarity_eval = SimilarityEvaluator(model_config=model_config)\n",
    "f1score_eval = F1ScoreEvaluator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-02 15:46:12 +1000][promptflow._sdk._orchestrator.run_submitter][INFO] - Upload run to cloud: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt flow service has started...\n",
      "You can view the traces in local from http://127.0.0.1:23333/v1.0/ui/traces/?#run=n0_prerequisites_20240802_154612_455515\n",
      "You can view the traces in azure portal since trace destination is set to: azureml://subscriptions/3c8972d9-f541-46b2-b70b-d81baba3595d/resourceGroups/secure-ai-rg/providers/Microsoft.MachineLearningServices/workspaces/krbock-0635. The link will be printed once the run is finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-02 15:46:16 +1000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run n0_prerequisites_20240802_154612_455515, log path: /home/krbock/.promptflow/.runs/n0_prerequisites_20240802_154612_455515/logs.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:46:39 +1000   30491 execution.bulk     INFO     Process 30521 terminated.\n",
      "2024-08-02 15:46:39 +1000   30491 execution.bulk     INFO     Process 30533 terminated.\n",
      "2024-08-02 15:46:39 +1000   30491 execution.bulk     WARNING  Process 30526 had been terminated.\n",
      "2024-08-02 15:46:39 +1000   30491 execution.bulk     WARNING  Process 30516 had been terminated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-02 15:46:40 +1000][promptflow._sdk._orchestrator.run_submitter][INFO] - Uploading run 'n0_prerequisites_20240802_154612_455515' to cloud...\n",
      "[2024-08-02 15:47:00 +1000][promptflow._sdk._orchestrator.run_submitter][INFO] - Updating run 'n0_prerequisites_20240802_154612_455515' portal url to 'https://ai.azure.com/projectflows/trace/run/n0_prerequisites_20240802_154612_455515/details?wsid=/subscriptions/3c8972d9-f541-46b2-b70b-d81baba3595d/resourcegroups/secure-ai-rg/providers/Microsoft.MachineLearningServices/workspaces/krbock-0635'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portal url: https://ai.azure.com/projectflows/trace/run/n0_prerequisites_20240802_154612_455515/details?wsid=/subscriptions/3c8972d9-f541-46b2-b70b-d81baba3595d/resourcegroups/secure-ai-rg/providers/Microsoft.MachineLearningServices/workspaces/krbock-0635\n",
      "2024-08-02 15:46:16 +1000   29780 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2024-08-02 15:46:16 +1000   29780 execution          WARNING  Starting run without column mapping may lead to unexpected results. Please consult the following documentation for more information: https://aka.ms/pf/column-mapping\n",
      "2024-08-02 15:46:16 +1000   29780 execution.bulk     INFO     Set process count to 4 by taking the minimum value among the factors of {'default_worker_count': 4, 'row_count': 13}.\n",
      "2024-08-02 15:46:24 +1000   29780 execution.bulk     INFO     Process name(ForkProcess-4:4)-Process id(30533)-Line number(0) start execution.\n",
      "2024-08-02 15:46:24 +1000   29780 execution.bulk     INFO     Process name(ForkProcess-4:1)-Process id(30516)-Line number(1) start execution.\n",
      "2024-08-02 15:46:24 +1000   29780 execution.bulk     INFO     Process name(ForkProcess-4:2)-Process id(30521)-Line number(2) start execution.\n",
      "2024-08-02 15:46:24 +1000   29780 execution.bulk     INFO     Process name(ForkProcess-4:3)-Process id(30526)-Line number(3) start execution.\n",
      "2024-08-02 15:46:27 +1000   29780 execution.bulk     INFO     Process name(ForkProcess-4:2)-Process id(30521)-Line number(2) completed.\n",
      "2024-08-02 15:46:27 +1000   29780 execution.bulk     INFO     Process name(ForkProcess-4:2)-Process id(30521)-Line number(4) start execution.\n",
      "2024-08-02 15:46:27 +1000   29780 execution.bulk     INFO     Finished 1 / 13 lines.\n",
      "2024-08-02 15:46:27 +1000   29780 execution.bulk     INFO     Average execution time for completed lines: 11.07 seconds. Estimated time for incomplete lines: 132.84 seconds.\n",
      "2024-08-02 15:46:29 +1000   29780 execution.bulk     INFO     Process name(ForkProcess-4:2)-Process id(30521)-Line number(4) completed.\n",
      "2024-08-02 15:46:29 +1000   29780 execution.bulk     INFO     Process name(ForkProcess-4:2)-Process id(30521)-Line number(5) start execution.\n",
      "2024-08-02 15:46:30 +1000   29780 execution.bulk     INFO     Finished 2 / 13 lines.\n",
      "2024-08-02 15:46:30 +1000   29780 execution.bulk     INFO     Average execution time for completed lines: 7.04 seconds. Estimated time for incomplete lines: 77.44 seconds.\n",
      "2024-08-02 15:46:31 +1000   29780 execution.bulk     INFO     Process name(ForkProcess-4:3)-Process id(30526)-Line number(3) completed.\n",
      "2024-08-02 15:46:31 +1000   29780 execution.bulk     INFO     Process name(ForkProcess-4:3)-Process id(30526)-Line number(6) start execution.\n",
      "2024-08-02 15:46:31 +1000   29780 execution.bulk     INFO     Finished 3 / 13 lines.\n",
      "2024-08-02 15:46:31 +1000   29780 execution.bulk     INFO     Average execution time for completed lines: 5.03 seconds. Estimated time for incomplete lines: 50.3 seconds.\n",
      "2024-08-02 15:46:31 +1000   29780 execution.bulk     INFO     Process name(ForkProcess-4:4)-Process id(30533)-Line number(0) completed.\n",
      "2024-08-02 15:46:31 +1000   29780 execution.bulk     INFO     Process name(ForkProcess-4:4)-Process id(30533)-Line number(7) start execution.\n",
      "2024-08-02 15:46:32 +1000   29780 execution.bulk     INFO     Process name(ForkProcess-4:1)-Process id(30516)-Line number(1) completed.\n",
      "2024-08-02 15:46:32 +1000   29780 execution.bulk     INFO     Process name(ForkProcess-4:1)-Process id(30516)-Line number(8) start execution.\n",
      "2024-08-02 15:46:32 +1000   29780 execution.bulk     INFO     Finished 5 / 13 lines.\n",
      "2024-08-02 15:46:32 +1000   29780 execution.bulk     INFO     Average execution time for completed lines: 3.22 seconds. Estimated time for incomplete lines: 25.76 seconds.\n",
      "2024-08-02 15:46:33 +1000   29780 execution.bulk     INFO     Process name(ForkProcess-4:3)-Process id(30526)-Line number(6) completed.\n",
      "2024-08-02 15:46:33 +1000   29780 execution.bulk     INFO     Process name(ForkProcess-4:3)-Process id(30526)-Line number(9) start execution.\n",
      "2024-08-02 15:46:33 +1000   29780 execution.bulk     INFO     Finished 6 / 13 lines.\n",
      "2024-08-02 15:46:33 +1000   29780 execution.bulk     INFO     Average execution time for completed lines: 2.85 seconds. Estimated time for incomplete lines: 19.95 seconds.\n",
      "2024-08-02 15:46:34 +1000   29780 execution.bulk     INFO     Process name(ForkProcess-4:1)-Process id(30516)-Line number(8) completed.\n",
      "2024-08-02 15:46:34 +1000   29780 execution.bulk     INFO     Process name(ForkProcess-4:1)-Process id(30516)-Line number(10) start execution.\n",
      "2024-08-02 15:46:34 +1000   29780 execution.bulk     INFO     Finished 7 / 13 lines.\n",
      "2024-08-02 15:46:34 +1000   29780 execution.bulk     INFO     Average execution time for completed lines: 2.58 seconds. Estimated time for incomplete lines: 15.48 seconds.\n",
      "2024-08-02 15:46:34 +1000   29780 execution.bulk     INFO     Process name(ForkProcess-4:4)-Process id(30533)-Line number(7) completed.\n",
      "2024-08-02 15:46:34 +1000   29780 execution.bulk     INFO     Process name(ForkProcess-4:4)-Process id(30533)-Line number(11) start execution.\n",
      "2024-08-02 15:46:35 +1000   29780 execution.bulk     INFO     Finished 8 / 13 lines.\n",
      "2024-08-02 15:46:35 +1000   29780 execution.bulk     INFO     Average execution time for completed lines: 2.39 seconds. Estimated time for incomplete lines: 11.95 seconds.\n",
      "2024-08-02 15:46:36 +1000   29780 execution.bulk     INFO     Process name(ForkProcess-4:3)-Process id(30526)-Line number(9) completed.\n",
      "2024-08-02 15:46:36 +1000   29780 execution.bulk     INFO     Process name(ForkProcess-4:3)-Process id(30526)-Line number(12) start execution.\n",
      "2024-08-02 15:46:36 +1000   29780 execution.bulk     INFO     Finished 9 / 13 lines.\n",
      "2024-08-02 15:46:36 +1000   29780 execution.bulk     INFO     Average execution time for completed lines: 2.23 seconds. Estimated time for incomplete lines: 8.92 seconds.\n",
      "2024-08-02 15:46:37 +1000   29780 execution.bulk     INFO     Process name(ForkProcess-4:2)-Process id(30521)-Line number(5) completed.\n",
      "2024-08-02 15:46:37 +1000   29780 execution.bulk     INFO     Process name(ForkProcess-4:1)-Process id(30516)-Line number(10) completed.\n",
      "2024-08-02 15:46:37 +1000   29780 execution.bulk     INFO     Finished 11 / 13 lines.\n",
      "2024-08-02 15:46:37 +1000   29780 execution.bulk     INFO     Average execution time for completed lines: 1.92 seconds. Estimated time for incomplete lines: 3.84 seconds.\n",
      "2024-08-02 15:46:37 +1000   29780 execution.bulk     INFO     Process name(ForkProcess-4:3)-Process id(30526)-Line number(12) completed.\n",
      "2024-08-02 15:46:38 +1000   29780 execution.bulk     INFO     Finished 12 / 13 lines.\n",
      "2024-08-02 15:46:38 +1000   29780 execution.bulk     INFO     Average execution time for completed lines: 1.84 seconds. Estimated time for incomplete lines: 1.84 seconds.\n",
      "2024-08-02 15:46:39 +1000   29780 execution.bulk     INFO     Process name(ForkProcess-4:4)-Process id(30533)-Line number(11) completed.\n",
      "2024-08-02 15:46:39 +1000   29780 execution.bulk     INFO     Finished 13 / 13 lines.\n",
      "2024-08-02 15:46:39 +1000   29780 execution.bulk     INFO     Average execution time for completed lines: 1.78 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2024-08-02 15:46:39 +1000   29780 execution.bulk     INFO     The thread monitoring the process [30521-ForkProcess-4:2] will be terminated.\n",
      "2024-08-02 15:46:39 +1000   29780 execution.bulk     INFO     The thread monitoring the process [30526-ForkProcess-4:3] will be terminated.\n",
      "2024-08-02 15:46:39 +1000   29780 execution.bulk     INFO     The thread monitoring the process [30533-ForkProcess-4:4] will be terminated.\n",
      "2024-08-02 15:46:39 +1000   30521 execution.bulk     INFO     The process [30521] has received a terminate signal.\n",
      "2024-08-02 15:46:39 +1000   30526 execution.bulk     INFO     The process [30526] has received a terminate signal.\n",
      "2024-08-02 15:46:39 +1000   29780 execution.bulk     INFO     The thread monitoring the process [30516-ForkProcess-4:1] will be terminated.\n",
      "2024-08-02 15:46:39 +1000   30533 execution.bulk     INFO     The process [30533] has received a terminate signal.\n",
      "2024-08-02 15:46:39 +1000   30516 execution.bulk     INFO     The process [30516] has received a terminate signal.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"n0_prerequisites_20240802_154612_455515\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2024-08-02 15:46:12.454873+10:00\"\n",
      "Duration: \"0:00:28.256792\"\n",
      "Output path: \"/home/krbock/.promptflow/.runs/n0_prerequisites_20240802_154612_455515\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-02 15:47:03 +1000][flowinvoker][INFO] - Getting connections from pf client with provider from args: local...\n",
      "[2024-08-02 15:47:03 +1000][flowinvoker][INFO] - Getting connections from pf client with provider from args: local...\n",
      "[2024-08-02 15:47:03 +1000][flowinvoker][INFO] - Promptflow get connections successfully. keys: dict_keys([])\n",
      "[2024-08-02 15:47:03 +1000][flowinvoker][INFO] - Promptflow get connections successfully. keys: dict_keys([])\n",
      "[2024-08-02 15:47:03 +1000][flowinvoker][INFO] - Promptflow executor starts initializing...\n",
      "[2024-08-02 15:47:03 +1000][flowinvoker][INFO] - Promptflow executor starts initializing...\n",
      "[2024-08-02 15:47:03 +1000][flowinvoker][INFO] - Promptflow executor initiated successfully.\n",
      "[2024-08-02 15:47:03 +1000][flowinvoker][INFO] - Promptflow executor initiated successfully.\n",
      "[2024-08-02 15:47:03 +1000][flowinvoker][INFO] - Validating flow input with data {'answer': \"When looking for a camping table that holds the most weight, you'll want to consider heavy-duty options made from robust materials like steel or aluminum. Some of the top-rated high-capacity camping tables include:\\n\\n1. **Lifetime Height Adjustable Folding Table**: Known for its durability, this table can support up to 1,000 pounds evenly distributed.\\n\\n2. **REI Co-op Camp Prep Table**: While it is lightweight and easy to transport, it can hold a substantial amount of weight, generally around 100-150 pounds.\\n\\n3. **Coleman Pack-Away 4-in-1 Adjustable Folding Table**: This versatile table can support approximately 300 pounds, depending on the configuration.\\n\\n4. **GCI Outdoor Slim-Fold Cook Station**: It’s specifically designed for heavy-duty use and can support around 50 pounds on its main tabletop and 35 pounds on each side table.\\n\\n5. **ALPS Mountaineering Dining Table**: This table can handle up to 100 pounds, making it suitable for various camping needs.\\n\\nAlways check the manufacturer's specifications for the maximum weight capacity and ensure that the table meets your specific needs.\", 'ground_truth': 'The Adventure Dining Table has a higher weight capacity than all of the other camping tables mentioned'}\n",
      "[2024-08-02 15:47:03 +1000][flowinvoker][INFO] - Execute flow with data {'answer': \"When looking for a camping table that holds the most weight, you'll want to consider heavy-duty options made from robust materials like steel or aluminum. Some of the top-rated high-capacity camping tables include:\\n\\n1. **Lifetime Height Adjustable Folding Table**: Known for its durability, this table can support up to 1,000 pounds evenly distributed.\\n\\n2. **REI Co-op Camp Prep Table**: While it is lightweight and easy to transport, it can hold a substantial amount of weight, generally around 100-150 pounds.\\n\\n3. **Coleman Pack-Away 4-in-1 Adjustable Folding Table**: This versatile table can support approximately 300 pounds, depending on the configuration.\\n\\n4. **GCI Outdoor Slim-Fold Cook Station**: It’s specifically designed for heavy-duty use and can support around 50 pounds on its main tabletop and 35 pounds on each side table.\\n\\n5. **ALPS Mountaineering Dining Table**: This table can handle up to 100 pounds, making it suitable for various camping needs.\\n\\nAlways check the manufacturer's specifications for the maximum weight capacity and ensure that the table meets your specific needs.\", 'ground_truth': 'The Adventure Dining Table has a higher weight capacity than all of the other camping tables mentioned'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Start executing nodes in thread pool mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-02 15:47:03 +1000][flowinvoker][INFO] - Validating flow input with data {'answer': \"When looking for a highly waterproof tent, you'll want to consider the hydrostatic head rating, which measures the waterproofness of the tent fabric. A higher rating indicates better water resistance. Here are some key features to look for in a waterproof tent:\\n\\n1. **Hydrostatic Head Rating**: A rating of 1500 mm to 3000 mm is generally considered waterproof for tents. However, for heavy rain conditions, look for ratings above 3000 mm.\\n\\n2. **Material**: High-quality polyester or nylon with a durable waterproof coating (like polyurethane or silicone) tends to be more waterproof.\\n\\n3. **Seam Sealing**: Ensure the tent has fully taped or sealed seams to prevent water from leaking through the stitching.\\n\\n4. **Design Features**: Look for tents with a full-coverage rainfly, bathtub-style floors that extend a few inches up the walls, and well-ventilated designs to reduce condensation.\\n\\nSome popular waterproof tent models include:\\n- **The North Face VE 25**: Known for its durability and high waterproof rating.\\n- **MSR Hubba Hubba NX**: Offers excellent waterproofing and ventilation.\\n- **Big Agnes Copper Spur HV UL2**: Lightweight with good waterproof features.\\n- **Hilleberg Nallo GT**: High-end option with superior waterproofing and durability.\\n\\nRemember that even the most waterproof tent needs proper setup and maintenance to perform well in wet conditions.\", 'ground_truth': 'The Alpine Explorer Tent has the highest rainfly waterproof rating at 3000m'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Start to run 2 nodes with concurrency level 16.\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Current thread is not main thread, skip signal handler registration in AsyncNodesScheduler.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-02 15:47:03 +1000][flowinvoker][INFO] - Execute flow with data {'answer': \"When looking for a highly waterproof tent, you'll want to consider the hydrostatic head rating, which measures the waterproofness of the tent fabric. A higher rating indicates better water resistance. Here are some key features to look for in a waterproof tent:\\n\\n1. **Hydrostatic Head Rating**: A rating of 1500 mm to 3000 mm is generally considered waterproof for tents. However, for heavy rain conditions, look for ratings above 3000 mm.\\n\\n2. **Material**: High-quality polyester or nylon with a durable waterproof coating (like polyurethane or silicone) tends to be more waterproof.\\n\\n3. **Seam Sealing**: Ensure the tent has fully taped or sealed seams to prevent water from leaking through the stitching.\\n\\n4. **Design Features**: Look for tents with a full-coverage rainfly, bathtub-style floors that extend a few inches up the walls, and well-ventilated designs to reduce condensation.\\n\\nSome popular waterproof tent models include:\\n- **The North Face VE 25**: Known for its durability and high waterproof rating.\\n- **MSR Hubba Hubba NX**: Offers excellent waterproofing and ventilation.\\n- **Big Agnes Copper Spur HV UL2**: Lightweight with good waterproof features.\\n- **Hilleberg Nallo GT**: High-end option with superior waterproofing and durability.\\n\\nRemember that even the most waterproof tent needs proper setup and maintenance to perform well in wet conditions.\", 'ground_truth': 'The Alpine Explorer Tent has the highest rainfly waterproof rating at 3000m'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Executing node validate_inputs. node run id: cda7e023-24f3-405a-8878-3bd06eff0b64_validate_inputs_3fbcc017-c1ec-45e3-8588-dafe6026bd38\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Start executing nodes in thread pool mode.\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Node validate_inputs completes.\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Start to run 2 nodes with concurrency level 16.\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     The node 'compute_f1_score' will be executed because the activate condition is met, i.e. '${validate_inputs.output}' is equal to 'True'.\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Current thread is not main thread, skip signal handler registration in AsyncNodesScheduler.\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Executing node compute_f1_score. node run id: cda7e023-24f3-405a-8878-3bd06eff0b64_compute_f1_score_ea43ded5-3795-40e5-95a7-88e59d5c6720\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Executing node validate_inputs. node run id: 68d191c7-2f5a-4bb0-9207-8cccffee5dd5_validate_inputs_91193ac6-ab7f-432c-b1c2-2dbcb61c1720\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Node compute_f1_score completes.\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Node validate_inputs completes.\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     The node 'compute_f1_score' will be executed because the activate condition is met, i.e. '${validate_inputs.output}' is equal to 'True'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-02 15:47:03 +1000][flowinvoker][INFO] - Validating flow input with data {'answer': \"I don't have real-time access to current prices, but the cost of TrailWalker Hiking Shoes can vary based on the retailer, model, and any ongoing promotions or discounts. For the most accurate and up-to-date pricing, I recommend checking the official website of the brand, online marketplaces like Amazon or eBay, or visiting a local outdoor or sporting goods store. Is there a specific model or retailer you're interested in?\", 'ground_truth': '$110'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Executing node compute_f1_score. node run id: 68d191c7-2f5a-4bb0-9207-8cccffee5dd5_compute_f1_score_d3137d17-087c-4942-b078-32352a3d1d05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-02 15:47:03 +1000][flowinvoker][INFO] - Execute flow with data {'answer': \"I don't have real-time access to current prices, but the cost of TrailWalker Hiking Shoes can vary based on the retailer, model, and any ongoing promotions or discounts. For the most accurate and up-to-date pricing, I recommend checking the official website of the brand, online marketplaces like Amazon or eBay, or visiting a local outdoor or sporting goods store. Is there a specific model or retailer you're interested in?\", 'ground_truth': '$110'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Node compute_f1_score completes.\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Start executing nodes in thread pool mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-02 15:47:03 +1000][flowinvoker][INFO] - Validating flow input with data {'answer': \"Taking proper care of your trailwalker hiking shoes will help extend their lifespan and maintain their performance. Here are some tips for caring for your hiking shoes:\\n\\n1. **Cleaning**:\\n   - **After Each Hike**: Remove dirt and debris by brushing off the shoes with a soft brush or cloth.\\n   - **Deep Cleaning**: If the shoes are very dirty, use a mixture of water and mild soap to scrub them gently. Avoid using harsh chemicals or detergents as they can damage the materials.\\n   - **Rinse**: Rinse the shoes thoroughly with clean water to remove any soap residue.\\n\\n2. **Drying**:\\n   - **Air Dry**: Let the shoes air dry naturally in a well-ventilated area away from direct sunlight and heat sources (like radiators or heaters), which can cause the materials to crack or warp.\\n   - **Remove Insoles and Laces**: Take out the insoles and laces to allow all parts of the shoe to dry completely.\\n   - **Stuff with Newspaper**: To help absorb moisture and maintain the shape, stuff the shoes with newspaper.\\n\\n3. **Conditioning**:\\n   - **Leather Shoes**: If your hiking shoes have leather components, apply a leather conditioner periodically to keep the leather supple and prevent it from drying out.\\n   - **Fabric Shoes**: Fabric and synthetic materials generally do not need conditioning but do check the manufacturer's recommendations.\\n\\n4. **Waterproofing**:\\n   - **Reapply Waterproofing**: Over time, the waterproofing treatment on your shoes can wear off. Use a waterproofing spray or treatment appropriate for the material of your shoes to reapply protection as needed.\\n\\n5. **Storage**:\\n   - **Cool and Dry Place**: Store your hiking shoes in a cool, dry place away from direct sunlight.\\n   - **Avoid Compression**: Do not store them in a way that compresses or deforms the shoes. Ensure they are in their natural shape.\\n\\n6. **Inspection and Repairs**:\\n   - **Regularly Inspect**: Check your shoes regularly for signs of wear and tear, such as damaged stitching, worn-out soles, or broken eyelets.\\n   - **Repair Promptly**: Address any minor issues immediately to prevent them from becoming major problems. For significant repairs, consider taking them to a professional cobbler who specializes in hiking footwear.\\n\\nBy following these care tips, your trailwalker hiking shoes will stay in good condition and provide you with reliable performance on your adventures.\", 'ground_truth': 'After each use, remove any dirt or debris by brushing or wiping the shoes with a damp cloth.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Start to run 2 nodes with concurrency level 16.\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Current thread is not main thread, skip signal handler registration in AsyncNodesScheduler.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-02 15:47:03 +1000][flowinvoker][INFO] - Execute flow with data {'answer': \"Taking proper care of your trailwalker hiking shoes will help extend their lifespan and maintain their performance. Here are some tips for caring for your hiking shoes:\\n\\n1. **Cleaning**:\\n   - **After Each Hike**: Remove dirt and debris by brushing off the shoes with a soft brush or cloth.\\n   - **Deep Cleaning**: If the shoes are very dirty, use a mixture of water and mild soap to scrub them gently. Avoid using harsh chemicals or detergents as they can damage the materials.\\n   - **Rinse**: Rinse the shoes thoroughly with clean water to remove any soap residue.\\n\\n2. **Drying**:\\n   - **Air Dry**: Let the shoes air dry naturally in a well-ventilated area away from direct sunlight and heat sources (like radiators or heaters), which can cause the materials to crack or warp.\\n   - **Remove Insoles and Laces**: Take out the insoles and laces to allow all parts of the shoe to dry completely.\\n   - **Stuff with Newspaper**: To help absorb moisture and maintain the shape, stuff the shoes with newspaper.\\n\\n3. **Conditioning**:\\n   - **Leather Shoes**: If your hiking shoes have leather components, apply a leather conditioner periodically to keep the leather supple and prevent it from drying out.\\n   - **Fabric Shoes**: Fabric and synthetic materials generally do not need conditioning but do check the manufacturer's recommendations.\\n\\n4. **Waterproofing**:\\n   - **Reapply Waterproofing**: Over time, the waterproofing treatment on your shoes can wear off. Use a waterproofing spray or treatment appropriate for the material of your shoes to reapply protection as needed.\\n\\n5. **Storage**:\\n   - **Cool and Dry Place**: Store your hiking shoes in a cool, dry place away from direct sunlight.\\n   - **Avoid Compression**: Do not store them in a way that compresses or deforms the shoes. Ensure they are in their natural shape.\\n\\n6. **Inspection and Repairs**:\\n   - **Regularly Inspect**: Check your shoes regularly for signs of wear and tear, such as damaged stitching, worn-out soles, or broken eyelets.\\n   - **Repair Promptly**: Address any minor issues immediately to prevent them from becoming major problems. For significant repairs, consider taking them to a professional cobbler who specializes in hiking footwear.\\n\\nBy following these care tips, your trailwalker hiking shoes will stay in good condition and provide you with reliable performance on your adventures.\", 'ground_truth': 'After each use, remove any dirt or debris by brushing or wiping the shoes with a damp cloth.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Executing node validate_inputs. node run id: 8a131f1b-fc2b-4280-8c25-41c4057ca478_validate_inputs_6231dbc1-5a23-4b99-9538-8a327c95cebe\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Start executing nodes in thread pool mode.\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Node validate_inputs completes.\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Start to run 2 nodes with concurrency level 16.\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     The node 'compute_f1_score' will be executed because the activate condition is met, i.e. '${validate_inputs.output}' is equal to 'True'.\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Current thread is not main thread, skip signal handler registration in AsyncNodesScheduler.\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Executing node compute_f1_score. node run id: 8a131f1b-fc2b-4280-8c25-41c4057ca478_compute_f1_score_5a726619-15de-4b34-96b9-070c37d1593e\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Executing node validate_inputs. node run id: adb678d9-3c6b-4f58-ac4d-aa73b5d22fa6_validate_inputs_2fb8630d-41cc-408f-97e4-20de757547b0\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Node compute_f1_score completes.\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Node validate_inputs completes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-02 15:47:03 +1000][flowinvoker][INFO] - Validating flow input with data {'answer': 'TrailMaster tents are often associated with the brand Wenzel. Wenzel is known for producing a variety of outdoor gear, including tents, and the TrailMaster model is one of their offerings. Is there anything specific you would like to know about the TrailMaster tent or Wenzel products in general?', 'ground_truth': 'OutdoorLiving'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     The node 'compute_f1_score' will be executed because the activate condition is met, i.e. '${validate_inputs.output}' is equal to 'True'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-02 15:47:03 +1000][flowinvoker][INFO] - Execute flow with data {'answer': 'TrailMaster tents are often associated with the brand Wenzel. Wenzel is known for producing a variety of outdoor gear, including tents, and the TrailMaster model is one of their offerings. Is there anything specific you would like to know about the TrailMaster tent or Wenzel products in general?', 'ground_truth': 'OutdoorLiving'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Executing node compute_f1_score. node run id: adb678d9-3c6b-4f58-ac4d-aa73b5d22fa6_compute_f1_score_c9a8424f-0abb-4418-9323-84836109d3c9\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Start executing nodes in thread pool mode.\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Node compute_f1_score completes.\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Start to run 2 nodes with concurrency level 16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-02 15:47:03 +1000][flowinvoker][INFO] - Validating flow input with data {'answer': 'Carrying the TrailMaster tent is straightforward with a few steps:\\n\\n1. **Disassemble and Pack**: After using the tent, ensure it is fully disassembled. Remove all stakes, poles, and other components, and fold the tent fabric neatly.\\n\\n2. **Storage Bag**: Most TrailMaster tents come with a storage bag. Carefully place the folded tent fabric, poles, stakes, and any additional components into the bag.\\n\\n3. **Secure the Bag**: Once everything is inside, securely close the bag, ensuring the zippers or drawstrings are fastened to prevent anything from falling out.\\n\\n4. **Use Carrying Straps**: The storage bag typically has carrying straps. Use these straps to carry the bag over your shoulder or by hand.\\n\\n5. **Distribute Weight**: If you are hiking or traveling a long distance, consider distributing the weight of the tent among your group members or within your backpack to make it easier to carry.\\n\\n6. **Backpack Integration**: Some backpacks have external straps or compartments designed to carry tents. Secure the tent bag to your backpack if this is an option for you.\\n\\nBy following these steps, you can carry your TrailMaster tent comfortably and efficiently on your adventures.', 'ground_truth': ' Carry bag included for convenient storage and transportation'}\n",
      "[2024-08-02 15:47:03 +1000][flowinvoker][INFO] - Execute flow with data {'answer': 'Carrying the TrailMaster tent is straightforward with a few steps:\\n\\n1. **Disassemble and Pack**: After using the tent, ensure it is fully disassembled. Remove all stakes, poles, and other components, and fold the tent fabric neatly.\\n\\n2. **Storage Bag**: Most TrailMaster tents come with a storage bag. Carefully place the folded tent fabric, poles, stakes, and any additional components into the bag.\\n\\n3. **Secure the Bag**: Once everything is inside, securely close the bag, ensuring the zippers or drawstrings are fastened to prevent anything from falling out.\\n\\n4. **Use Carrying Straps**: The storage bag typically has carrying straps. Use these straps to carry the bag over your shoulder or by hand.\\n\\n5. **Distribute Weight**: If you are hiking or traveling a long distance, consider distributing the weight of the tent among your group members or within your backpack to make it easier to carry.\\n\\n6. **Backpack Integration**: Some backpacks have external straps or compartments designed to carry tents. Secure the tent bag to your backpack if this is an option for you.\\n\\nBy following these steps, you can carry your TrailMaster tent comfortably and efficiently on your adventures.', 'ground_truth': ' Carry bag included for convenient storage and transportation'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Current thread is not main thread, skip signal handler registration in AsyncNodesScheduler.\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Executing node validate_inputs. node run id: cb25c0f6-60cf-4ef2-8009-ce7caa4ed057_validate_inputs_db1211f0-d6a3-4b15-8724-7bba21927b98\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Start executing nodes in thread pool mode.\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Node validate_inputs completes.\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Start to run 2 nodes with concurrency level 16.\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     The node 'compute_f1_score' will be executed because the activate condition is met, i.e. '${validate_inputs.output}' is equal to 'True'.\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Current thread is not main thread, skip signal handler registration in AsyncNodesScheduler.\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Executing node compute_f1_score. node run id: cb25c0f6-60cf-4ef2-8009-ce7caa4ed057_compute_f1_score_2f373d48-f0fc-4c6f-80eb-3a029867fcd1\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Executing node validate_inputs. node run id: c7cd1c16-a210-4f0d-84b4-5cd597e818d1_validate_inputs_fb70b311-e6c4-4a9d-969a-cc87fb741ed7\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Node compute_f1_score completes.\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Node validate_inputs completes.\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     The node 'compute_f1_score' will be executed because the activate condition is met, i.e. '${validate_inputs.output}' is equal to 'True'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-02 15:47:03 +1000][flowinvoker][INFO] - Validating flow input with data {'answer': 'It looks like you\\'re asking about the floor area, but I\\'m not sure if you\\'re referring to a specific context or if there\\'s a typo. Could you clarify what you mean by \"Floor Area\"? Are you asking how to calculate the floor area of a room, building, or something else?', 'ground_truth': '80 square feet'}\n",
      "[2024-08-02 15:47:03 +1000][flowinvoker][INFO] - Execute flow with data {'answer': 'It looks like you\\'re asking about the floor area, but I\\'m not sure if you\\'re referring to a specific context or if there\\'s a typo. Could you clarify what you mean by \"Floor Area\"? Are you asking how to calculate the floor area of a room, building, or something else?', 'ground_truth': '80 square feet'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Executing node compute_f1_score. node run id: c7cd1c16-a210-4f0d-84b4-5cd597e818d1_compute_f1_score_90db4ae2-33e5-487a-80dd-25122feccdaa\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Node compute_f1_score completes.\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Start executing nodes in thread pool mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-02 15:47:03 +1000][flowinvoker][INFO] - Validating flow input with data {'answer': \"TrailBlaze Hiking Pants are typically made from a blend of materials designed to offer durability, flexibility, and comfort. Common materials include nylon, polyester, and spandex. Nylon provides durability and resistance to abrasion, polyester offers moisture-wicking properties, and spandex adds stretch for ease of movement. The exact material composition can vary depending on the specific model and brand, so it's always a good idea to check the product details for the most accurate information.\", 'ground_truth': 'Made of high-quality nylon fabric'}\n",
      "[2024-08-02 15:47:03 +1000][flowinvoker][INFO] - Execute flow with data {'answer': \"TrailBlaze Hiking Pants are typically made from a blend of materials designed to offer durability, flexibility, and comfort. Common materials include nylon, polyester, and spandex. Nylon provides durability and resistance to abrasion, polyester offers moisture-wicking properties, and spandex adds stretch for ease of movement. The exact material composition can vary depending on the specific model and brand, so it's always a good idea to check the product details for the most accurate information.\", 'ground_truth': 'Made of high-quality nylon fabric'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Start to run 2 nodes with concurrency level 16.\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Start executing nodes in thread pool mode.\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Current thread is not main thread, skip signal handler registration in AsyncNodesScheduler.\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Start to run 2 nodes with concurrency level 16.\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Executing node validate_inputs. node run id: 1defb51a-299f-49ad-94df-5f33c2e6053f_validate_inputs_2a8a4746-5bac-412a-b121-c16061c07d71\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Current thread is not main thread, skip signal handler registration in AsyncNodesScheduler.\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Node validate_inputs completes.\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Executing node validate_inputs. node run id: a0fa4e84-9cf4-4582-b78f-35b38c055755_validate_inputs_bdec7c99-9fe1-4185-ae76-81c860fb1c5f\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     The node 'compute_f1_score' will be executed because the activate condition is met, i.e. '${validate_inputs.output}' is equal to 'True'.\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Node validate_inputs completes.\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Executing node compute_f1_score. node run id: 1defb51a-299f-49ad-94df-5f33c2e6053f_compute_f1_score_335818df-f7f6-498b-aefd-2a853aa23cfe\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     The node 'compute_f1_score' will be executed because the activate condition is met, i.e. '${validate_inputs.output}' is equal to 'True'.\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Node compute_f1_score completes.\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Executing node compute_f1_score. node run id: a0fa4e84-9cf4-4582-b78f-35b38c055755_compute_f1_score_d43636ca-04ce-49ef-b6ae-c5124dadc50b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-02 15:47:03 +1000][flowinvoker][INFO] - Validating flow input with data {'answer': \"I'm not sure about the specific product details of TrailBlaze Hiking Pants. However, hiking pants in general often come in a variety of colors like khaki, olive green, black, navy blue, and sometimes even more vibrant colors like red or orange. To get the most accurate information, you might want to check the manufacturer's website or a retailer that sells them.\", 'ground_truth': 'Khaki'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Node compute_f1_score completes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-02 15:47:03 +1000][flowinvoker][INFO] - Execute flow with data {'answer': \"I'm not sure about the specific product details of TrailBlaze Hiking Pants. However, hiking pants in general often come in a variety of colors like khaki, olive green, black, navy blue, and sometimes even more vibrant colors like red or orange. To get the most accurate information, you might want to check the manufacturer's website or a retailer that sells them.\", 'ground_truth': 'Khaki'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Start executing nodes in thread pool mode.\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Start to run 2 nodes with concurrency level 16.\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Current thread is not main thread, skip signal handler registration in AsyncNodesScheduler.\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Executing node validate_inputs. node run id: 3543175e-3f38-41ac-a810-9b060eb4243b_validate_inputs_05a264c7-39c4-4328-857e-dc7cbef54e9c\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Node validate_inputs completes.\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     The node 'compute_f1_score' will be executed because the activate condition is met, i.e. '${validate_inputs.output}' is equal to 'True'.\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Executing node compute_f1_score. node run id: 3543175e-3f38-41ac-a810-9b060eb4243b_compute_f1_score_a9f24264-f659-46aa-a194-e9a58f117cc5\n",
      "2024-08-02 15:47:03 +1000   29780 execution.flow     INFO     Node compute_f1_score completes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-02 15:47:06 +1000][flowinvoker][INFO] - Validating flow input with data {'answer': \"Warranty policies can vary depending on the manufacturer or retailer from which you purchased the TrailBlaze pants. Generally, most warranties are non-transferable and apply only to the original purchaser. However, it's always a good idea to check the specific warranty terms provided by the manufacturer or retailer. You can usually find this information on their website or in the documentation that came with the product. If you're still unsure, contacting their customer service directly would be the best way to get a definitive answer.\", 'ground_truth': 'he warranty is non-transferable and applies only to the original purchaser of the TrailBlaze Hiking Pants. It is valid only when the product is purchased from an authorized retailer.'}\n",
      "[2024-08-02 15:47:06 +1000][flowinvoker][INFO] - Execute flow with data {'answer': \"Warranty policies can vary depending on the manufacturer or retailer from which you purchased the TrailBlaze pants. Generally, most warranties are non-transferable and apply only to the original purchaser. However, it's always a good idea to check the specific warranty terms provided by the manufacturer or retailer. You can usually find this information on their website or in the documentation that came with the product. If you're still unsure, contacting their customer service directly would be the best way to get a definitive answer.\", 'ground_truth': 'he warranty is non-transferable and applies only to the original purchaser of the TrailBlaze Hiking Pants. It is valid only when the product is purchased from an authorized retailer.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:47:06 +1000   29780 execution.flow     INFO     Start executing nodes in thread pool mode.\n",
      "2024-08-02 15:47:06 +1000   29780 execution.flow     INFO     Start to run 2 nodes with concurrency level 16.\n",
      "2024-08-02 15:47:06 +1000   29780 execution.flow     INFO     Current thread is not main thread, skip signal handler registration in AsyncNodesScheduler.\n",
      "2024-08-02 15:47:06 +1000   29780 execution.flow     INFO     Executing node validate_inputs. node run id: d07f08f0-4102-46bd-ac14-b3f5df3e2c33_validate_inputs_3b98460d-b6cc-4f7a-a1d7-db383fee0b82\n",
      "2024-08-02 15:47:06 +1000   29780 execution.flow     INFO     Node validate_inputs completes.\n",
      "2024-08-02 15:47:06 +1000   29780 execution.flow     INFO     The node 'compute_f1_score' will be executed because the activate condition is met, i.e. '${validate_inputs.output}' is equal to 'True'.\n",
      "2024-08-02 15:47:06 +1000   29780 execution.flow     INFO     Executing node compute_f1_score. node run id: d07f08f0-4102-46bd-ac14-b3f5df3e2c33_compute_f1_score_4b8e9a7e-ab63-446c-864d-5a7ee1ff180d\n",
      "2024-08-02 15:47:06 +1000   29780 execution.flow     INFO     Node compute_f1_score completes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-02 15:47:06 +1000][flowinvoker][INFO] - Validating flow input with data {'answer': \"The warranty period for TrailBlaze pants can vary depending on the brand and retailer you purchased them from. Most outdoor clothing brands offer a warranty ranging from one year to a lifetime against manufacturing defects. It's best to check the warranty information provided by the specific brand or retailer where you bought the pants. If you have that information on hand, I can help you look it up!\", 'ground_truth': ' The TrailBlaze Hiking Pants are backed by a 1-year limited warranty from the date of purchase.'}\n",
      "[2024-08-02 15:47:06 +1000][flowinvoker][INFO] - Execute flow with data {'answer': \"The warranty period for TrailBlaze pants can vary depending on the brand and retailer you purchased them from. Most outdoor clothing brands offer a warranty ranging from one year to a lifetime against manufacturing defects. It's best to check the warranty information provided by the specific brand or retailer where you bought the pants. If you have that information on hand, I can help you look it up!\", 'ground_truth': ' The TrailBlaze Hiking Pants are backed by a 1-year limited warranty from the date of purchase.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:47:06 +1000   29780 execution.flow     INFO     Start executing nodes in thread pool mode.\n",
      "2024-08-02 15:47:06 +1000   29780 execution.flow     INFO     Start to run 2 nodes with concurrency level 16.\n",
      "2024-08-02 15:47:06 +1000   29780 execution.flow     INFO     Current thread is not main thread, skip signal handler registration in AsyncNodesScheduler.\n",
      "2024-08-02 15:47:06 +1000   29780 execution.flow     INFO     Executing node validate_inputs. node run id: 49349024-b5a3-4fdd-8dfd-2ab2a36e3234_validate_inputs_822f0801-d089-43dd-a6ff-69caeb4e1f9f\n",
      "2024-08-02 15:47:06 +1000   29780 execution.flow     INFO     Node validate_inputs completes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-02 15:47:06 +1000][flowinvoker][INFO] - Validating flow input with data {'answer': \"The PowerBurner Camping Stove is typically made from a combination of durable materials designed to withstand high temperatures and outdoor conditions. Common materials include stainless steel for the burner and body, aluminum for lightweight components, and heat-resistant plastics for handles and knobs. These materials ensure the stove is both robust and portable, making it ideal for camping and outdoor use. Always check the specific model's specifications for precise material details.\", 'ground_truth': 'Stainless Steel'}\n",
      "[2024-08-02 15:47:06 +1000][flowinvoker][INFO] - Execute flow with data {'answer': \"The PowerBurner Camping Stove is typically made from a combination of durable materials designed to withstand high temperatures and outdoor conditions. Common materials include stainless steel for the burner and body, aluminum for lightweight components, and heat-resistant plastics for handles and knobs. These materials ensure the stove is both robust and portable, making it ideal for camping and outdoor use. Always check the specific model's specifications for precise material details.\", 'ground_truth': 'Stainless Steel'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:47:06 +1000   29780 execution.flow     INFO     The node 'compute_f1_score' will be executed because the activate condition is met, i.e. '${validate_inputs.output}' is equal to 'True'.\n",
      "2024-08-02 15:47:06 +1000   29780 execution.flow     INFO     Start executing nodes in thread pool mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-02 15:47:06 +1000][flowinvoker][INFO] - Validating flow input with data {'answer': \"Yes, that's correct! France is a country located in Western Europe. It's known for its rich history, culture, cuisine, and landmarks such as the Eiffel Tower and the Louvre Museum. Is there something specific you'd like to know about France?\", 'ground_truth': 'Sorry, I can only truth questions related to outdoor/camping gear and equipment'}\n",
      "[2024-08-02 15:47:06 +1000][flowinvoker][INFO] - Execute flow with data {'answer': \"Yes, that's correct! France is a country located in Western Europe. It's known for its rich history, culture, cuisine, and landmarks such as the Eiffel Tower and the Louvre Museum. Is there something specific you'd like to know about France?\", 'ground_truth': 'Sorry, I can only truth questions related to outdoor/camping gear and equipment'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-02 15:47:06 +1000   29780 execution.flow     INFO     Executing node compute_f1_score. node run id: 49349024-b5a3-4fdd-8dfd-2ab2a36e3234_compute_f1_score_b2515c57-5ee0-467f-a674-83abf9d7f546\n",
      "2024-08-02 15:47:06 +1000   29780 execution.flow     INFO     Start to run 2 nodes with concurrency level 16.\n",
      "2024-08-02 15:47:06 +1000   29780 execution.flow     INFO     Node compute_f1_score completes.\n",
      "2024-08-02 15:47:06 +1000   29780 execution.flow     INFO     Start executing nodes in thread pool mode.\n",
      "2024-08-02 15:47:06 +1000   29780 execution.flow     INFO     Current thread is not main thread, skip signal handler registration in AsyncNodesScheduler.\n",
      "2024-08-02 15:47:06 +1000   29780 execution.flow     INFO     Start to run 2 nodes with concurrency level 16.\n",
      "2024-08-02 15:47:06 +1000   29780 execution.flow     INFO     Executing node validate_inputs. node run id: 59d474ea-8fd2-4e51-a65f-8ca367a0019e_validate_inputs_07d71376-6257-4278-8a38-ea2720fc0a4e\n",
      "2024-08-02 15:47:06 +1000   29780 execution.flow     INFO     Current thread is not main thread, skip signal handler registration in AsyncNodesScheduler.\n",
      "2024-08-02 15:47:06 +1000   29780 execution.flow     INFO     Node validate_inputs completes.\n",
      "2024-08-02 15:47:06 +1000   29780 execution.flow     INFO     Executing node validate_inputs. node run id: b416e15f-5153-4b79-8702-212e321c76d3_validate_inputs_8662f313-de9c-44fc-aa8f-4446ddf20fe1\n",
      "2024-08-02 15:47:06 +1000   29780 execution.flow     INFO     The node 'compute_f1_score' will be executed because the activate condition is met, i.e. '${validate_inputs.output}' is equal to 'True'.\n",
      "2024-08-02 15:47:06 +1000   29780 execution.flow     INFO     Executing node compute_f1_score. node run id: 59d474ea-8fd2-4e51-a65f-8ca367a0019e_compute_f1_score_40906022-68d1-4172-a84d-8032ff8e7ce2\n",
      "2024-08-02 15:47:06 +1000   29780 execution.flow     INFO     Node compute_f1_score completes.\n",
      "2024-08-02 15:47:06 +1000   29780 execution.flow     INFO     Node validate_inputs completes.\n",
      "2024-08-02 15:47:06 +1000   29780 execution.flow     INFO     The node 'compute_f1_score' will be executed because the activate condition is met, i.e. '${validate_inputs.output}' is equal to 'True'.\n",
      "2024-08-02 15:47:06 +1000   29780 execution.flow     INFO     Executing node compute_f1_score. node run id: b416e15f-5153-4b79-8702-212e321c76d3_compute_f1_score_8bc402a8-3393-4d56-b4a5-7250fbedc9b4\n",
      "2024-08-02 15:47:06 +1000   29780 execution.flow     INFO     Node compute_f1_score completes.\n"
     ]
    }
   ],
   "source": [
    "from promptflow.evals.evaluate import evaluate\n",
    "\n",
    "result = evaluate(\n",
    "    evaluation_name=\"rai-workshop-test\", #name your evaluation to view in AI Studio\n",
    "    data='../data/evaluation_dataset.jsonl', # provide your data here - must be string\n",
    "    target=llm_tool,\n",
    "    evaluators={\n",
    "        #\"relevance\": relevance_eval,\n",
    "        \"coherence\": coherence_eval,\n",
    "        #\"groundedness\": groundedness_eval,\n",
    "        \"fluency\": fluency_eval,\n",
    "        \"similarity\": similarity_eval,\n",
    "        \"f1score\": f1score_eval\n",
    "\n",
    "    },\n",
    "    # column mapping\n",
    "    evaluator_config={\n",
    "        \"default\": {\n",
    "            \"questions\": \"${data.question)\", #column of data providing input to model\n",
    "            #\"contexts\": \"${data.context}\", #column of data providing context for each input\n",
    "            \"answer\": \"${target.answer}\", #column of data providing output from model\n",
    "            \"ground_truth\":\"${data.truth}\" #column of data providing ground truth answer, optional for default metrics\n",
    "        }\n",
    "    },\n",
    "    # Optionally provide your AI Studio project information to track your evaluation results in your Azure AI studio project\n",
    "    azure_ai_project = azure_ai_project,\n",
    "    # Optionally provide an output path to dump a json of metric summary, row level data and metric and studio URL\n",
    "    output_path=output_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Retrieval Augmented Generation (RAG) application using Promptflow SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the RAG pattern to validate our model against ground-truth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data\n",
    "This sample uses files from the folder data/ in this repo. You can clone this repo or copy this folder to make sure you have access to these files when running the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'Which tent is the most waterproof?',\n",
       "  'truth': 'The Alpine Explorer Tent has the highest rainfly waterproof rating at 3000m'},\n",
       " {'question': 'Which camping table holds the most weight?',\n",
       "  'truth': 'The Adventure Dining Table has a higher weight capacity than all of the other camping tables mentioned'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from earlier step\n",
    "# import json\n",
    "# def load_jsonl(path):\n",
    "#     with open(path, \"r\") as f:\n",
    "#         return [json.loads(line) for line in f.readlines()]\n",
    "\n",
    "#mydata = load_jsonl('../data/evaluation_dataset.jsonl')\n",
    "\n",
    "mydata[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an local FAISS index from your local files\n",
    "https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/index-build-consume-sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the FAISS package with\n",
    "```\n",
    "\"Please install it with `pip install faiss-gpu` (for CUDA supported GPU) \"\n",
    "    \"or `pip install faiss-cpu` (depending on Python version).\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the AI Studio project\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "client=MLClient(\n",
    "    DefaultAzureCredential(), \n",
    "    subscription_id=os.environ.get(\"SUBSCRIPTION_ID\"),\n",
    "    resource_group_name=os.environ.get(\"RESOURCE_GROUP_NAME\"),\n",
    "    workspace_name=os.environ.get(\"PROJECT_NAME\") \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use AIStudio's Azure OpenAI connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.rag.config import ConnectionConfig\n",
    "# embedding_model_config = ConnectionConfig(\n",
    "#     subscription_id = os.environ.get(\"SUBSCRIPTION_ID\"),\n",
    "#     resource_group_name = os.environ.get(\"RESOURCE_GROUP_NAME\"),\n",
    "#     workspace_name = os.environ.get(\"PROJECT_NAME\"),\n",
    "#     connection_name = \"Default_AzureOpenAI\"\n",
    "# )\n",
    "\n",
    "embedding_model_config = ConnectionConfig(\n",
    "    subscription_id = os.environ.get(\"SUBSCRIPTION_ID\"),\n",
    "    resource_group_name = os.environ.get(\"RESOURCE_GROUP_NAME\"),\n",
    "    workspace_name = os.environ.get(\"PROJECT_NAME\"),\n",
    "    connection_name = os.environ.get(\"AISTUDIO_AOAI_CONNECTION_NAME\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crack and chunk files from local path: ../data/product-info/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:azureml.rag.connections:Using ml_client base_url: https://management.azure.com, original_base_url: https://management.azure.com.\n",
      "INFO:azureml.rag.connections:Parsed Connection: /subscriptions/3c8972d9-f541-46b2-b70b-d81baba3595d/resourceGroups/secure-ai-rg/providers/Microsoft.MachineLearningServices/workspaces/krbock-0635/connections/mssecureai4034688619_aoai\n",
      "INFO:azureml.rag.connections:Got connection: /subscriptions/3c8972d9-f541-46b2-b70b-d81baba3595d/resourceGroups/secure-ai-rg/providers/Microsoft.MachineLearningServices/workspaces/krbock-0635/connections/mssecureai4034688619_aoai as <class 'azure.ai.ml.entities._workspace.connections.connection_subtypes.AzureOpenAIConnection'>.\n",
      "INFO:azureml.rag.connections:Getting workspace connection: mssecureai4034688619_aoai, with input credential: <class 'NoneType'>.\n",
      "INFO:azureml.rag.connections:Getting workspace connection via MLClient with auth: <class 'azure.identity._credentials.default.DefaultAzureCredential'>, subscription_id: 3c8972d9-f541-46b2-b70b-d81baba3595d, resource_group_name: secure-ai-rg, workspace_name: krbock-0635.\n",
      "INFO:azureml.rag.connections:Using ml_client base_url: https://management.azure.com, original_base_url: https://management.azure.com.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start embedding using connection with id = /subscriptions/3c8972d9-f541-46b2-b70b-d81baba3595d/resourceGroups/secure-ai-rg/providers/Microsoft.MachineLearningServices/workspaces/krbock-0635/connections/mssecureai4034688619_aoai\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:azureml.rag.connections:Parsed Connection: /subscriptions/3c8972d9-f541-46b2-b70b-d81baba3595d/resourceGroups/secure-ai-rg/providers/Microsoft.MachineLearningServices/workspaces/krbock-0635/connections/mssecureai4034688619_aoai\n",
      "INFO:azureml.rag.connections:Got connection: /subscriptions/3c8972d9-f541-46b2-b70b-d81baba3595d/resourceGroups/secure-ai-rg/providers/Microsoft.MachineLearningServices/workspaces/krbock-0635/connections/mssecureai4034688619_aoai as <class 'azure.ai.ml.entities._workspace.connections.connection_subtypes.AzureOpenAIConnection'>.\n",
      "INFO:azureml.rag.connections:The connection 'mssecureai4034688619_aoai' is a <class 'azure.ai.ml.entities._workspace.connections.connection_subtypes.AzureOpenAIConnection'> with api_key auth type.\n",
      "INFO:azureml.rag.azureml.rag.documents:[DocumentChunksIterator::filter_extensions] Filtered 0 files out of 20\n",
      "INFO:azureml.rag.azureml.rag.documents.cracking:[DocumentChunksIterator::crack_documents] Total time to load files: 0.0014028549194335938\n",
      "{\n",
      "  \".txt\": 0.0,\n",
      "  \".md\": 20.0,\n",
      "  \".html\": 0.0,\n",
      "  \".htm\": 0.0,\n",
      "  \".py\": 0.0,\n",
      "  \".pdf\": 0.0,\n",
      "  \".ppt\": 0.0,\n",
      "  \".pptx\": 0.0,\n",
      "  \".doc\": 0.0,\n",
      "  \".docx\": 0.0,\n",
      "  \".xls\": 0.0,\n",
      "  \".xlsx\": 0.0,\n",
      "  \".csv\": 0.0,\n",
      "  \".json\": 0.0\n",
      "}\n",
      "INFO:azureml.rag.azureml.rag.documents.chunking:[DocumentChunksIterator::split_documents] Total time to split 20 documents into 75 chunks: 0.6921420097351074\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_8.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_8.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_8.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_8.md3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_13.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_13.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_13.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_4.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_4.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_4.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_4.md3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_9.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_9.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_9.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_9.md3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_20.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_20.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_20.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_20.md3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_15.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_15.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_15.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_15.md3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_12.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_12.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_12.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_6.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_6.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_6.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_6.md3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_19.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_19.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_19.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_19.md3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_18.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_18.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_18.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_18.md3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_10.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_10.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_10.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_16.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_16.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_16.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_16.md3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_16.md4\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_3.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_3.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_3.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_3.md3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_17.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_17.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_17.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_17.md3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_11.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_11.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_11.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_11.md3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_14.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_14.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_14.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_7.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_7.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_7.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_1.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_1.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_1.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_1.md3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_5.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_5.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_5.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_5.md3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_2.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_2.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_2.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Documents to embed: 75\n",
      "Documents reused: 0\n",
      "WARNING:azureml.rag.embeddings.openai:Warning: model not found. Using cl100k_base encoding.\n",
      "INFO:azureml.rag.embeddings.openai:Attempt 0 to embed 16 documents.\n",
      "INFO:azureml.rag.embeddings.openai:Attempt 0 to embed 16 documents.\n",
      "INFO:azureml.rag.embeddings.openai:Attempt 0 to embed 16 documents.\n",
      "INFO:azureml.rag.embeddings.openai:Attempt 0 to embed 16 documents.\n",
      "INFO:azureml.rag.embeddings.openai:Attempt 0 to embed 11 documents.\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Building faiss index\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_8.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_8.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_8.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_8.md3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_13.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_13.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_13.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_4.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_4.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_4.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_4.md3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_9.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_9.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_9.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_9.md3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_20.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_20.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_20.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_20.md3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_15.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_15.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_15.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_15.md3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_12.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_12.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_12.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_6.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_6.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_6.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_6.md3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_19.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_19.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_19.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_19.md3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_18.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_18.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_18.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_18.md3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_10.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_10.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_10.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_16.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_16.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_16.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_16.md3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_16.md4\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_3.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_3.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_3.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_3.md3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_17.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_17.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_17.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_17.md3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_11.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_11.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_11.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_11.md3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_14.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_14.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_14.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_7.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_7.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_7.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_1.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_1.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_1.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_1.md3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_5.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_5.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_5.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_5.md3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_2.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_2.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Adding document: product_info_2.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Built index from 75 documents and 75 chunks, took 0.1962 seconds\n",
      "INFO:azureml.rag.connections:The connection 'mssecureai4034688619_aoai' is a <class 'azure.ai.ml.entities._workspace.connections.connection_subtypes.AzureOpenAIConnection'> with api_key auth type.\n",
      "WARNING:langchain_community.vectorstores.faiss:`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Saving index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created index at ../data/product-info-faiss-index-mlindex\n"
     ]
    }
   ],
   "source": [
    "from promptflow.rag.config import LocalSource, EmbeddingsModelConfig\n",
    "from promptflow.rag import build_index\n",
    "\n",
    "faiss_index_name = \"product-info-faiss-index\"\n",
    "embedding_output_dir = \"../data\"\n",
    "\n",
    "# build the index\n",
    "faiss_index=build_index(\n",
    "    name=faiss_index_name,  # name of your index\n",
    "    vector_store=\"faiss\",  # the type of vector store\n",
    "    embeddings_model_config=EmbeddingsModelConfig(\n",
    "        model_name=os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL\"),\n",
    "        deployment_name=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"),\n",
    "        connection_config=embedding_model_config\n",
    "    ),\n",
    "    input_source=LocalSource(input_data=\"../data/product-info/\"),  # the location of your file/folders\n",
    "    #index_config=LocalSource(input_data=\"../data/product-info/\"\n",
    "        #ai_search_index_name=\"<your-index-name>\" + \"-aoai-store\", # the name of the index store inside the azure ai search service\n",
    "    #),\n",
    "    tokens_per_chunk = 800, # Optional field - Maximum number of tokens per chunk\n",
    "    token_overlap_across_chunks = 0, # Optional field - Number of tokens to overlap between chunks\n",
    "    embeddings_cache_path=embedding_output_dir, # Optional field - Path to store embeddings cache\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consume index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'faiss_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpromptflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrag\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_langchain_retriever_from_index\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Get the OpenAI embedded Index\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#retriever=get_langchain_retriever_from_index(faiss_index)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m retriever\u001b[38;5;241m=\u001b[39mget_langchain_retriever_from_index(\u001b[43mfaiss_index\u001b[49m)\n\u001b[1;32m      6\u001b[0m retriever\u001b[38;5;241m.\u001b[39mget_relevant_documents(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhich tent is the most waterproof\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'faiss_index' is not defined"
     ]
    }
   ],
   "source": [
    "from promptflow.rag import get_langchain_retriever_from_index\n",
    "\n",
    "# Get the OpenAI embedded Index\n",
    "#retriever=get_langchain_retriever_from_index(faiss_index)\n",
    "retriever=get_langchain_retriever_from_index(faiss_index)\n",
    "retriever.get_relevant_documents(\"Which tent is the most waterproof\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Register Index (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the AI Studio project\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "client=MLClient(\n",
    "    DefaultAzureCredential(), \n",
    "    subscription_id=os.environ.get(\"SUBSCRIPTION_ID\"),\n",
    "    resource_group_name=os.environ.get(\"RESOURCE_GROUP_NAME\"),\n",
    "    workspace_name=os.environ.get(\"PROJECT_NAME\") \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Index\n",
    "\n",
    "# register the index with Azure OpenAI embeddings\n",
    "client.indexes.create_or_update(\n",
    "    Index(name=faiss_index_name + \"aoai\", \n",
    "          path=faiss_index, \n",
    "          version=\"1\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: Use Azure AI Search to create an index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.rag.config import ConnectionConfig\n",
    "embedding_model_config = ConnectionConfig(\n",
    "    subscription_id = os.environ.get(\"SUBSCRIPTION_ID\"),\n",
    "    resource_group_name = os.environ.get(\"RESOURCE_GROUP_NAME\"),\n",
    "    workspace_name = os.environ.get(\"PROJECT_NAME\"),\n",
    "    connection_name = os.environ.get(\"AISTUDIO_AOAI_CONNECTION_NAME\"),\n",
    ")\n",
    "\n",
    "ais_model_config = ConnectionConfig(\n",
    "    subscription_id = os.environ.get(\"SUBSCRIPTION_ID\"),\n",
    "    resource_group_name = os.environ.get(\"RESOURCE_GROUP_NAME\"),\n",
    "    workspace_name = os.environ.get(\"PROJECT_NAME\"),\n",
    "    connection_name = os.environ.get(\"AISTUDIO_AIS_CONNECTION_NAME\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the AI Studio project\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "client=MLClient(\n",
    "    DefaultAzureCredential(), \n",
    "    subscription_id=os.environ.get(\"SUBSCRIPTION_ID\"),\n",
    "    resource_group_name=os.environ.get(\"RESOURCE_GROUP_NAME\"),\n",
    "    workspace_name=os.environ.get(\"PROJECT_NAME\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## from https://learn.microsoft.com/en-us/azure/ai-studio/tutorials/copilot-sdk-build-rag\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml.entities import Index\n",
    "\n",
    "from promptflow.rag.config import (\n",
    "    LocalSource,\n",
    "    AzureAISearchConfig,\n",
    "    EmbeddingsModelConfig,\n",
    "    ConnectionConfig,\n",
    ")\n",
    "from promptflow.rag import build_index\n",
    "\n",
    "client = MLClient(\n",
    "    DefaultAzureCredential(),\n",
    "    os.getenv(\"AZURE_SUBSCRIPTION_ID\"),\n",
    "    os.getenv(\"AZURE_RESOURCE_GROUP\"),\n",
    "    os.getenv(\"AZUREAI_PROJECT_NAME\"),\n",
    ")\n",
    "import os\n",
    "\n",
    "# append directory of the current script to data directory\n",
    "script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "data_directory = os.path.join(script_dir, \"data/product-info/\")\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(data_directory):\n",
    "    files = os.listdir(data_directory)  # List all files in the directory\n",
    "    if files:\n",
    "        print(\n",
    "            f\"Data directory '{data_directory}' exists and contains {len(files)} files.\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Data directory '{data_directory}' exists but is empty.\")\n",
    "        exit()\n",
    "else:\n",
    "    print(f\"Data directory '{data_directory}' does not exist.\")\n",
    "    exit()\n",
    "\n",
    "index_name = \"tutorial-index\"  # your desired index name\n",
    "index_path = build_index(\n",
    "    name=index_name,  # name of your index\n",
    "    vector_store=\"azure_ai_search\",  # the type of vector store - in this case it is Azure AI Search. Users can also use \"azure_cognitive search\"\n",
    "    embeddings_model_config=EmbeddingsModelConfig(\n",
    "        model_name=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"),\n",
    "        deployment_name=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"),\n",
    "        connection_config=ConnectionConfig(\n",
    "            subscription_id=client.subscription_id,\n",
    "            resource_group_name=client.resource_group_name,\n",
    "            workspace_name=client.workspace_name,\n",
    "            connection_name=os.getenv(\"AZURE_OPENAI_CONNECTION_NAME\"),\n",
    "        ),\n",
    "    ),\n",
    "    input_source=LocalSource(input_data=data_directory),  # the location of your files\n",
    "    index_config=AzureAISearchConfig(\n",
    "        ai_search_index_name=index_name,  # the name of the index store inside the azure ai search service\n",
    "        ai_search_connection_config=ConnectionConfig(\n",
    "            subscription_id=client.subscription_id,\n",
    "            resource_group_name=client.resource_group_name,\n",
    "            workspace_name=client.workspace_name,\n",
    "            connection_name=os.getenv(\"AZURE_SEARCH_CONNECTION_NAME\"),\n",
    "        ),\n",
    "    ),\n",
    "    tokens_per_chunk=800,  # Optional field - Maximum number of tokens per chunk\n",
    "    token_overlap_across_chunks=0,  # Optional field - Number of tokens to overlap between chunks\n",
    ")\n",
    "\n",
    "# register the index so that it shows up in the cloud project\n",
    "client.indexes.create_or_update(Index(name=index_name, path=index_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:azureml.rag.connections:Getting workspace connection: mssecureai4034688619_aoai, with input credential: <class 'NoneType'>.\n",
      "INFO:azureml.rag.connections:Getting workspace connection via MLClient with auth: <class 'azure.identity._credentials.default.DefaultAzureCredential'>, subscription_id: 3c8972d9-f541-46b2-b70b-d81baba3595d, resource_group_name: secure-ai-rg, workspace_name: krbock-0635.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crack and chunk files from local path: ../data/product-info/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Method connections: This is an experimental method, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "INFO:azureml.rag.connections:Using ml_client base_url: https://management.azure.com, original_base_url: https://management.azure.com.\n",
      "INFO:azureml.rag.connections:Parsed Connection: /subscriptions/3c8972d9-f541-46b2-b70b-d81baba3595d/resourceGroups/secure-ai-rg/providers/Microsoft.MachineLearningServices/workspaces/krbock-0635/connections/mssecureai4034688619_aoai\n",
      "INFO:azureml.rag.connections:Got connection: /subscriptions/3c8972d9-f541-46b2-b70b-d81baba3595d/resourceGroups/secure-ai-rg/providers/Microsoft.MachineLearningServices/workspaces/krbock-0635/connections/mssecureai4034688619_aoai as <class 'azure.ai.ml.entities._workspace.connections.connection_subtypes.AzureOpenAIConnection'>.\n",
      "INFO:azureml.rag.connections:Getting workspace connection: mssecureai4034688619_aoai, with input credential: <class 'NoneType'>.\n",
      "INFO:azureml.rag.connections:Getting workspace connection via MLClient with auth: <class 'azure.identity._credentials.default.DefaultAzureCredential'>, subscription_id: 3c8972d9-f541-46b2-b70b-d81baba3595d, resource_group_name: secure-ai-rg, workspace_name: krbock-0635.\n",
      "INFO:azureml.rag.connections:Using ml_client base_url: https://management.azure.com, original_base_url: https://management.azure.com.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start embedding using connection with id = /subscriptions/3c8972d9-f541-46b2-b70b-d81baba3595d/resourceGroups/secure-ai-rg/providers/Microsoft.MachineLearningServices/workspaces/krbock-0635/connections/mssecureai4034688619_aoai\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:azureml.rag.connections:Parsed Connection: /subscriptions/3c8972d9-f541-46b2-b70b-d81baba3595d/resourceGroups/secure-ai-rg/providers/Microsoft.MachineLearningServices/workspaces/krbock-0635/connections/mssecureai4034688619_aoai\n",
      "INFO:azureml.rag.connections:Got connection: /subscriptions/3c8972d9-f541-46b2-b70b-d81baba3595d/resourceGroups/secure-ai-rg/providers/Microsoft.MachineLearningServices/workspaces/krbock-0635/connections/mssecureai4034688619_aoai as <class 'azure.ai.ml.entities._workspace.connections.connection_subtypes.AzureOpenAIConnection'>.\n",
      "INFO:azureml.rag.connections:The connection 'mssecureai4034688619_aoai' is a <class 'azure.ai.ml.entities._workspace.connections.connection_subtypes.AzureOpenAIConnection'> with api_key auth type.\n",
      "INFO:azureml.rag.azureml.rag.documents:[DocumentChunksIterator::filter_extensions] Filtered 0 files out of 20\n",
      "INFO:azureml.rag.azureml.rag.documents.cracking:[DocumentChunksIterator::crack_documents] Total time to load files: 0.0006465911865234375\n",
      "{\n",
      "  \".txt\": 0.0,\n",
      "  \".md\": 20.0,\n",
      "  \".html\": 0.0,\n",
      "  \".htm\": 0.0,\n",
      "  \".py\": 0.0,\n",
      "  \".pdf\": 0.0,\n",
      "  \".ppt\": 0.0,\n",
      "  \".pptx\": 0.0,\n",
      "  \".doc\": 0.0,\n",
      "  \".docx\": 0.0,\n",
      "  \".xls\": 0.0,\n",
      "  \".xlsx\": 0.0,\n",
      "  \".csv\": 0.0,\n",
      "  \".json\": 0.0\n",
      "}\n",
      "INFO:azureml.rag.azureml.rag.documents.chunking:[DocumentChunksIterator::split_documents] Total time to split 20 documents into 75 chunks: 0.41516876220703125\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_8.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_8.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_8.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_8.md3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_13.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_13.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_13.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_4.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_4.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_4.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_4.md3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_9.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_9.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_9.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_9.md3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_20.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_20.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_20.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_20.md3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_15.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_15.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_15.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_15.md3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_12.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_12.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_12.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_6.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_6.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_6.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_6.md3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_19.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_19.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_19.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_19.md3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_18.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_18.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_18.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_18.md3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_10.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_10.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_10.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_16.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_16.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_16.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_16.md3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_16.md4\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_3.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_3.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_3.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_3.md3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_17.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_17.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_17.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_17.md3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_11.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_11.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_11.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_11.md3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_14.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_14.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_14.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_7.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_7.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_7.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_1.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_1.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_1.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_1.md3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_5.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_5.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_5.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_5.md3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_2.md0\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_2.md1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing document: product_info_2.md2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Documents to embed: 75\n",
      "Documents reused: 0\n",
      "WARNING:azureml.rag.embeddings.openai:Warning: model not found. Using cl100k_base encoding.\n",
      "INFO:azureml.rag.embeddings.openai:Attempt 0 to embed 16 documents.\n",
      "INFO:azureml.rag.embeddings.openai:Attempt 0 to embed 16 documents.\n",
      "INFO:azureml.rag.embeddings.openai:Attempt 0 to embed 16 documents.\n",
      "INFO:azureml.rag.embeddings.openai:Attempt 0 to embed 16 documents.\n",
      "INFO:azureml.rag.embeddings.openai:Attempt 0 to embed 11 documents.\n",
      "INFO:azureml.rag.connections:Getting workspace connection: mssecureaisearch, with input credential: <class 'NoneType'>.\n",
      "INFO:azureml.rag.connections:Getting workspace connection via MLClient with auth: <class 'azure.identity._credentials.default.DefaultAzureCredential'>, subscription_id: 3c8972d9-f541-46b2-b70b-d81baba3595d, resource_group_name: secure-ai-rg, workspace_name: krbock-0635.\n",
      "INFO:azureml.rag.connections:Using ml_client base_url: https://management.azure.com, original_base_url: https://management.azure.com.\n",
      "INFO:azureml.rag.connections:Parsed Connection: /subscriptions/3c8972d9-f541-46b2-b70b-d81baba3595d/resourceGroups/secure-ai-rg/providers/Microsoft.MachineLearningServices/workspaces/krbock-0635/connections/mssecureaisearch\n",
      "INFO:azureml.rag.connections:Got connection: /subscriptions/3c8972d9-f541-46b2-b70b-d81baba3595d/resourceGroups/secure-ai-rg/providers/Microsoft.MachineLearningServices/workspaces/krbock-0635/connections/mssecureaisearch as <class 'azure.ai.ml.entities._workspace.connections.connection_subtypes.AzureAISearchConnection'>.\n",
      "INFO:azureml.rag.update_acs:Updating ACS index\n",
      "INFO:azureml.rag.connections:Getting workspace connection: mssecureaisearch, with input credential: <class 'NoneType'>.\n",
      "INFO:azureml.rag.connections:Getting workspace connection via MLClient with auth: <class 'azure.identity._credentials.default.DefaultAzureCredential'>, subscription_id: 3c8972d9-f541-46b2-b70b-d81baba3595d, resource_group_name: secure-ai-rg, workspace_name: krbock-0635.\n",
      "INFO:azureml.rag.connections:Using ml_client base_url: https://management.azure.com, original_base_url: https://management.azure.com.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start creating index from embeddings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:azureml.rag.connections:Parsed Connection: /subscriptions/3c8972d9-f541-46b2-b70b-d81baba3595d/resourceGroups/secure-ai-rg/providers/Microsoft.MachineLearningServices/workspaces/krbock-0635/connections/mssecureaisearch\n",
      "INFO:azureml.rag.connections:Got connection: /subscriptions/3c8972d9-f541-46b2-b70b-d81baba3595d/resourceGroups/secure-ai-rg/providers/Microsoft.MachineLearningServices/workspaces/krbock-0635/connections/mssecureaisearch as <class 'azure.ai.ml.entities._workspace.connections.connection_subtypes.AzureAISearchConnection'>.\n",
      "INFO:azureml.rag.connections:The connection 'mssecureaisearch' is a <class 'azure.ai.ml.entities._workspace.connections.connection_subtypes.AzureAISearchConnection'> with api_key auth type.\n",
      "INFO:azureml.rag.update_acs:Using Index fields: {\n",
      "  \"content\": \"content\",\n",
      "  \"url\": \"url\",\n",
      "  \"filename\": \"filepath\",\n",
      "  \"title\": \"title\",\n",
      "  \"metadata\": \"meta_json_string\",\n",
      "  \"embedding\": \"contentVector\"\n",
      "}\n",
      "INFO:azureml.rag.update_acs:Ensuring search index product-info-ais-index-aoai-store exists\n",
      "INFO:azureml.rag.update_acs:Search index product-info-ais-index-aoai-store already exists, checking if this index contains all the required fields\n",
      "INFO:azureml.rag.update_acs:Documents include embeddings: True\n",
      "INFO:azureml.rag.azureml.rag.embeddings:0 documents from sources marked for deletion from IndexStoreType.ACS index, adding individual documents marked for deletion\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Total 0 documents marked for deletion from IndexStoreType.ACS index\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing documents from: product_info_8\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 38239 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 75699 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 114147 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 150658 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processed source: product_info_8\n",
      "Total Documents: 4\n",
      "Skipped: 0\n",
      "Added: 4\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing documents from: product_info_13\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 189301 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 227693 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 266099 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processed source: product_info_13\n",
      "Total Documents: 3\n",
      "Skipped: 0\n",
      "Added: 3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing documents from: product_info_4\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 304477 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 343197 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 381630 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 416967 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processed source: product_info_4\n",
      "Total Documents: 4\n",
      "Skipped: 0\n",
      "Added: 4\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing documents from: product_info_9\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 455660 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 494237 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 532037 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 569432 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processed source: product_info_9\n",
      "Total Documents: 4\n",
      "Skipped: 0\n",
      "Added: 4\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing documents from: product_info_20\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 607648 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 646281 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 684784 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 721341 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processed source: product_info_20\n",
      "Total Documents: 4\n",
      "Skipped: 0\n",
      "Added: 4\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing documents from: product_info_15\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 759171 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 797603 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 835900 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 871103 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processed source: product_info_15\n",
      "Total Documents: 4\n",
      "Skipped: 0\n",
      "Added: 4\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing documents from: product_info_12\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 909692 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 948309 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 985976 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processed source: product_info_12\n",
      "Total Documents: 3\n",
      "Skipped: 0\n",
      "Added: 3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing documents from: product_info_6\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 1024527 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 1062473 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 1100698 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 1138467 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processed source: product_info_6\n",
      "Total Documents: 4\n",
      "Skipped: 0\n",
      "Added: 4\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing documents from: product_info_19\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 1177097 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 1215735 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 1253782 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 1290156 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processed source: product_info_19\n",
      "Total Documents: 4\n",
      "Skipped: 0\n",
      "Added: 4\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing documents from: product_info_18\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 1328378 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 1366985 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 1405353 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 1441038 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processed source: product_info_18\n",
      "Total Documents: 4\n",
      "Skipped: 0\n",
      "Added: 4\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing documents from: product_info_10\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 1479386 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 1517833 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 1556555 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processed source: product_info_10\n",
      "Total Documents: 3\n",
      "Skipped: 0\n",
      "Added: 3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing documents from: product_info_16\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 1595072 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 1632782 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 1671505 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 1709860 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 1745628 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processed source: product_info_16\n",
      "Total Documents: 5\n",
      "Skipped: 0\n",
      "Added: 5\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing documents from: product_info_3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 1783930 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 1822517 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 1860921 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 1896492 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processed source: product_info_3\n",
      "Total Documents: 4\n",
      "Skipped: 0\n",
      "Added: 4\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing documents from: product_info_17\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 1934902 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 1973524 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 2011863 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 2048306 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processed source: product_info_17\n",
      "Total Documents: 4\n",
      "Skipped: 0\n",
      "Added: 4\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing documents from: product_info_11\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 2086608 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 2124950 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 2163415 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 2198446 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processed source: product_info_11\n",
      "Total Documents: 4\n",
      "Skipped: 0\n",
      "Added: 4\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing documents from: product_info_14\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 2236514 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 2274588 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 2312970 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processed source: product_info_14\n",
      "Total Documents: 3\n",
      "Skipped: 0\n",
      "Added: 3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing documents from: product_info_7\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 2351383 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 2389994 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 2426470 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processed source: product_info_7\n",
      "Total Documents: 3\n",
      "Skipped: 0\n",
      "Added: 3\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing documents from: product_info_1\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 2464678 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 2503093 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 2541430 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 2576996 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processed source: product_info_1\n",
      "Total Documents: 4\n",
      "Skipped: 0\n",
      "Added: 4\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing documents from: product_info_5\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 2615334 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 2653776 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 2691942 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 2729513 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processed source: product_info_5\n",
      "Total Documents: 4\n",
      "Skipped: 0\n",
      "Added: 4\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Processing documents from: product_info_2\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 2767941 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 2806627 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:The total size of documents to upload in this batch is 2844870 bytes\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Uploading 75 documents of total size of 2844870 to IndexStoreType.ACS index\n",
      "INFO:azureml.rag.azureml.rag.indexes.index_stores:[AzureCognitiveSearchStore][upload_documents] Processed 75 documents in 3.8761 seconds, 0 failed\n",
      "INFO:azureml.rag.update_acs:Status: indexing - Indexed Documents - 75/75 indexed_documents\n",
      "INFO:azureml.rag.update_acs:Status: indexing - Skipped Documents - 0/75 skipped_documents\n",
      "INFO:azureml.rag.azureml.rag.embeddings:Built index from 75 documents and 75 chunks, took 4.0379 seconds\n",
      "INFO:azureml.rag.update_acs:Built index\n",
      "INFO:azureml.rag.update_acs:Status: indexing - Finished Indexed Documents - 75/75 indexed_documents\n",
      "INFO:azureml.rag.update_acs:Status: indexing - Skipped Documents - 0/75 skipped_documents\n",
      "INFO:azureml.rag.update_acs:Writing MLIndex yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created index at ../data/product-info-ais-index-mlindex\n"
     ]
    }
   ],
   "source": [
    "from promptflow.rag.config import AzureAISearchConfig, EmbeddingsModelConfig, LocalSource\n",
    "from promptflow.rag import build_index\n",
    "\n",
    "ais_index_name = \"product-info-ais-index\"\n",
    "embedding_output_dir = \"../data\"\n",
    "\n",
    "local_index_aoai=build_index(\n",
    "    name=ais_index_name,  # name of your index\n",
    "    vector_store=\"azure_ai_search\",  # the type of vector store\n",
    "    embeddings_model_config=EmbeddingsModelConfig(\n",
    "        model_name=os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL\"),\n",
    "        deployment_name=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"),\n",
    "        connection_config=embedding_model_config\n",
    "    ),\n",
    "    input_source=LocalSource(input_data=\"../data/product-info/\"),  # the location of your file/folders\n",
    "    index_config=AzureAISearchConfig(\n",
    "        ai_search_index_name=\"product-info-ais-index\" + \"-aoai-store\", # the name of the index store inside the azure ai search service\n",
    "        ai_search_connection_config=ais_model_config\n",
    "    ),\n",
    "    tokens_per_chunk = 800, # Optional field - Maximum number of tokens per chunk\n",
    "    token_overlap_across_chunks = 0, # Optional field - Number of tokens to overlap between chunks\n",
    "    embeddings_cache_path=embedding_output_dir, # Optional field - Path to store embeddings cache\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consume the local index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:azureml.rag.mlindex:Get ACS credential for with credential:<class 'NoneType'>.\n",
      "INFO:azureml.rag.connections:The connection 'mssecureaisearch' is a <class 'azure.ai.ml.entities._workspace.connections.connection_subtypes.AzureAISearchConnection'> with api_key auth type.\n",
      "INFO:azureml.rag.connections:The connection 'mssecureai4034688619_aoai' is a <class 'azure.ai.ml.entities._workspace.connections.connection_subtypes.AzureOpenAIConnection'> with api_key auth type.\n",
      "INFO:azureml.rag.connections:The connection 'mssecureai4034688619_aoai' is a <class 'azure.ai.ml.entities._workspace.connections.connection_subtypes.AzureOpenAIConnection'> with api_key auth type.\n",
      "WARNING:azureml.rag.embeddings.openai:Warning: model not found. Using cl100k_base encoding.\n",
      "INFO:azureml.rag.embeddings.openai:Attempt 0 to embed 1 documents.\n",
      "/home/krbock/miniconda3/envs/pf-eval/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "WARNING:azureml.rag.embeddings.openai:Warning: model not found. Using cl100k_base encoding.\n",
      "INFO:azureml.rag.embeddings.openai:Attempt 0 to embed 1 documents.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='# Information about product item_number: 1\\n\\n5. **Resolution Options**:\\n   - Upon receipt of the warranty claim, our customer support team will assess the issue and determine the appropriate resolution.\\n   - Options may include repair, replacement of the defective parts, or, if necessary, replacement of the entire tent.\\n\\n6. **Limitations and Exclusions**:\\n   - Our warranty is non-transferable and applies only to the original purchaser of the TrailMaster X4 Tent.\\n   - The warranty does not cover any incidental or consequential damages resulting from the use or inability to use the tent.\\n   - Any unauthorized repairs or alterations void the warranty.\\n\\n### Contact Information\\n\\nIf you have any questions or need further assistance, please contact our customer support:\\n\\n- Customer Support Phone: +1-800-123-4567\\n- Customer Support Email: support@example.com\\n\\n## Return Policy\\n- **If Membership status \"None        \":**\\tReturns are accepted within 30 days of purchase, provided the tent is unused, undamaged and in its original packaging. Customer is responsible for the cost of return shipping. Once the returned item is received, a refund will be issued for the cost of the item minus a 10% restocking fee. If the item was damaged during shipping or if there is a defect, the customer should contact customer service within 7 days of receiving the item.\\n- **If Membership status \"Gold\":**\\tReturns are accepted within 60 days of purchase, provided the tent is unused, undamaged and in its original packaging. Free return shipping is provided. Once the returned item is received, a full refund will be issued. If the item was damaged during shipping or if there is a defect, the customer should contact customer service within 7 days of receiving the item.\\n- **If Membership status \"Platinum\":**\\tReturns are accepted within 90 days of purchase, provided the tent is unused, undamaged and in its original packaging. Free return shipping is provided, and a full refund will be issued. If the item was damaged during shipping or if there is a defect, the customer should contact customer service within 7 days of receiving the item.\\n\\n## Reviews\\n1) **Rating:** 5\\n   **Review:** I am extremely happy with my TrailMaster X4 Tent! It\\'s spacious, easy to set up, and kept me dry during a storm. The UV protection is a great addition too. Highly recommend it to anyone who loves camping!\\n\\n2) **Rating:** 3\\n   **Review:** I bought the TrailMaster X4 Tent, and while it\\'s waterproof and has a spacious interior, I found it a bit difficult to set up. It\\'s a decent tent, but I wish it were easier to assemble.\\n\\n3) **Rating:** 5\\n   **Review:** The TrailMaster X4 Tent is a fantastic investment for any serious camper. The easy setup and spacious interior make it perfect for extended trips, and the waterproof design kept us dry in heavy rain.\\n\\n4) **Rating:** 4\\n   **Review:** I like the TrailMaster X4 Tent, but I wish it came in more colors. It\\'s comfortable and has many useful features, but the green color just isn\\'t my favorite. Overall, it\\'s a good tent.\\n\\n5) **Rating:** 5\\n   **Review:** This tent is perfect for my family camping trips. The spacious interior and convenient storage pocket make it easy to stay organized. It\\'s also super easy to set up, making it a great addition to our gear.\\n\\n## FAQ\\n1) Can the TrailMaster X4 Tent be used in winter conditions?\\n   The TrailMaster X4 Tent is designed for 3-season use and may not be suitable for extreme winter conditions with heavy snow and freezing temperatures.', metadata={'source': {'title': 'Information about product item_number: 1', 'filename': 'product_info_1.md', 'url': 'product_info_1.md', 'mtime': 1708391642.699326, 'chunk_id': '2'}, 'stats': {'tiktokens': 767, 'chars': 3512, 'lines': 42}}),\n",
       " Document(page_content='# Information about product item_number: 15\\n\\n69) Can the SkyView 2-Person Tent withstand strong winds?\\n   The SkyView 2-Person Tent is designed with strong aluminum poles and reinforced guylines, ensuring stability and durability in windy conditions.\\n\\n70) Are there any storage options inside the SkyView 2-Person Tent?\\n   Yes, the SkyView 2-Person Tent features interior mesh pockets and a gear loft for keeping your belongings organized and easily accessible.', metadata={'source': {'title': 'Information about product item_number: 15', 'filename': 'product_info_15.md', 'url': 'product_info_15.md', 'mtime': 1708391647.749325, 'chunk_id': '3'}, 'stats': {'tiktokens': 99, 'chars': 461, 'lines': 7}}),\n",
       " Document(page_content='# Information about product item_number: 8\\n\\n# Information about product item_number: 8\\nAlpine Explorer Tent, price $350,\\n\\n## Brand\\nAlpineGear\\n\\n## Category\\nTents\\n\\n### Features\\n- Waterproof: Provides reliable protection against rain and moisture.\\n- Easy Setup: Simple and quick assembly process, making it convenient for camping.\\n- Room Divider: Includes a detachable divider to create separate living spaces within the tent.\\n- Excellent Ventilation: Multiple mesh windows and vents promote airflow and reduce condensation.\\n- Gear Loft: Built-in gear loft or storage pockets for organizing and storing camping gear.\\n\\n## Technical Specs\\n**Best Use**: Camping\\n**Capacity**: 8-person\\n**Season Rating**: 3-season\\n**Setup**: Freestanding\\n**Material**: Polyester\\n**Waterproof**: Yes\\n**Floor Area**: 120 square feet\\n**Peak Height**: 6.5 feet\\n**Number of Doors**: 2\\n**Color**: Orange\\n**Rainfly**: Included\\n**Rainfly Waterproof Rating**: 3000mm\\n**Tent Poles**: Aluminum\\n**Pole Diameter**: 12mm\\n**Ventilation**: Mesh panels and adjustable vents\\n**Interior Pockets**: 4 pockets\\n**Gear Loft**: Included\\n**Footprint**: Sold separately\\n**Guy Lines**: Reflective\\n**Stakes**: Aluminum\\n**Carry Bag**: Included\\n**Dimensions**: 12ft x 10ft x 7ft (Length x Width x Peak Height)\\n**Packed Size**: 24 inches x 10 inches\\n**Weight**: 17 lbs\\n\\n## Alpine Explorer Tent User Guide\\n\\nThank you for choosing the Alpine Explorer Tent. This user guide provides instructions on how to set up, use, and maintain your tent effectively. Please read this guide thoroughly before using the tent.\\n\\n### Package Contents\\n\\nEnsure that the package includes the following components:\\n\\n- Alpine Explorer Tent body\\n- Tent poles\\n- Rainfly\\n- Stakes and guy lines\\n- Carry bag\\n- User Guide\\n\\nIf any components are missing or damaged, please contact our customer support immediately.\\n\\n### Tent Setup\\n\\n**Step 1: Selecting a Suitable Location**\\n\\n- Find a level and clear area for pitching the tent.\\n- Remove any sharp objects or debris that could damage the tent floor.\\n\\n**Step 2: Unpacking and Organizing Components**\\n\\n- Lay out all the tent components on the ground.\\n- Familiarize yourself with each part, including the tent body, poles, rainfly, stakes, and guy lines.\\n\\n**Step 3: Assembling the Tent Poles**\\n\\n- Connect the tent poles according to their designated color codes or numbering.\\n- Slide the connected poles through the pole sleeves or attach them to the tent body clips.\\n\\n**Step 4: Setting up the Tent Body**\\n\\n- Begin at one end and raise the tent body by pushing up the poles.\\n- Ensure that the tent body is evenly stretched and centered.\\n- Secure the tent body to the ground using stakes and guy lines as needed.\\n\\n**Step 5: Attaching the Rainfly**\\n\\n- Spread the rainfly over the tent body.\\n- Attach the rainfly to the tent corners and secure it with the provided buckles or clips.\\n- Adjust the tension of the rainfly to ensure proper airflow and weather protection.\\n\\n**Step 6: Securing the Tent**\\n\\n- Stake down the tent corners and guy out the guy lines for additional stability.\\n- Adjust the tension of the guy lines to provide optimal stability and wind resistance.\\n\\n### Tent Takedown and Storage\\n\\n**Step 1: Removing Stakes and Guy Lines**\\n\\n- Remove all stakes from the ground.\\n- Untie or disconnect the guy lines from the tent and store them separately.\\n\\n**Step 2: Taking Down the Tent Body**', metadata={'source': {'title': 'Information about product item_number: 8', 'filename': 'product_info_8.md', 'url': 'product_info_8.md', 'mtime': 1708391645.3493257, 'chunk_id': '0'}, 'stats': {'tiktokens': 771, 'chars': 3353, 'lines': 103}}),\n",
       " Document(page_content=\"# Information about product item_number: 8\\n\\n- Start by collapsing the tent poles carefully.\\n- Remove the poles from the pole sleeves or clips.\\n- Collapse the tent body and fold it neatly.\\n\\n**Step 3: Disassembling the Tent Poles**\\n\\n- Disconnect and separate the individual pole sections.\\n- Store the poles in their designated bag or sleeve.\\n\\n**Step 4: Packing the Tent**\\n\\n- Fold the tent body tightly and place it in the carry bag.\\n- Ensure that the rainfly and any other components are also packed securely.\\n\\n### Tent Care and Maintenance\\n\\n- Clean the tent regularly with mild soap and water.\\n- Avoid using harsh chemicals or abrasive cleaners.\\n- Allow the tent to dry completely before storing it.\\n- Store the tent in a cool, dry place away from direct sunlight.\\n\\n### Contact Information\\n\\nIf you have any questions or need further assistance, please contact our customer support:\\n\\n- Customer Support Phone: +1-800-123-4567\\n- Customer Support Email: support@alpineexplorer.com\\n\\nWe hope you enjoy your Alpine Explorer Tent and have a great outdoor experience!\\n\\n## Cautions:\\n1. **Stable Setup**: Ensure the tent is pitched on level ground and avoid uneven or sloping terrain to maintain stability.\\n2. **Safe Location**: Choose a safe and secure location away from hazardous areas like cliffs, rivers, or flood-prone zones.\\n3. **Windproof**: Do not leave the tent unattended during strong winds or severe weather conditions; properly secure all guy lines and stakes for added stability.\\n4. **Flame-Free**: Never use open flames or heating equipment inside the tent to prevent fire hazards and damage to the tent material.\\n5. **Gentle Glide**: Avoid dragging or pulling the tent body on rough surfaces to prevent tears or abrasions; lift and move the tent with care.\\n6. **Dry and Mold-Free**: Ensure the tent is completely dry before storing to prevent mold and mildew growth; avoid packing a damp or wet tent.\\n7. **Sunshine Protection**: Avoid storing the tent in direct sunlight for extended periods to prevent UV damage to the fabric; find a cool and shaded storage area.\\n8. **Capacity Check**: Do not exceed the tent's recommended weight capacity to maintain its structural integrity and prevent potential damage.\\n9. **Sharp Object Safety**: Keep sharp objects and tools away from the tent to avoid punctures or fabric damage; handle the tent and its components with care.\\n10. **Original Structure**: Do not modify or alter the tent structure or components, as it may compromise the tent's performance and void the warranty.\\nBy following these cautionary guidelines, you can ensure safe and optimal use of the Alpine Explorer Tent.\\n\\n## Warranty Information:\", metadata={'source': {'title': 'Information about product item_number: 8', 'filename': 'product_info_8.md', 'url': 'product_info_8.md', 'mtime': 1708391645.3493257, 'chunk_id': '1'}, 'stats': {'tiktokens': 554, 'chars': 2657, 'lines': 46}})]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from promptflow.rag import get_langchain_retriever_from_index\n",
    "\n",
    "# Get the OpenAI embedded Index\n",
    "retriever=get_langchain_retriever_from_index(local_index_aoai)\n",
    "#retriever=get_langchain_retriever_from_index(\"product-info-ais-index\")\n",
    "retriever.get_relevant_documents(\"Which tent is the most waterproof\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Method indexes: This is an experimental method, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class Index: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created index 'product-info-ais-index'\n",
      "Cloud Path: azureml://subscriptions/3c8972d9-f541-46b2-b70b-d81baba3595d/resourcegroups/secure-ai-rg/workspaces/krbock-0635/datastores/workspaceblobstore/paths/LocalUpload/409b87928c3c49dbbc1daf85bfac8699/product-info-ais-index-mlindex\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Index\n",
    "# register the index so that it shows up in the project\n",
    "cloud_index = client.indexes.create_or_update(Index(name=ais_index_name, path=local_index_aoai))\n",
    "\n",
    "print(f\"Created index '{cloud_index.name}'\")\n",
    "print(f\"Cloud Path: {cloud_index.path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a Flow to evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Evaluator | question | answer | context | ground_truth |\n",
    "| --- | --- | --- | --- | --- | \n",
    "| GroundednessEvaluator | N/A | Required: String | Required: String | N/A |\n",
    "| RelevanceEvaluator | Required: String | Required: String | Required: String | N/A |\n",
    "| CoherenceEvaluator | Required: String | Required: String | N/A | N/A |\n",
    "| FluencyEvaluator | Required: String | Required: String | N/A | N/A |\n",
    "| SimilarityEvaluator | Required: String | Required: String | N/A | Required: String |\n",
    "| F1ScoreEvaluator | N/A | Required: String | N/A | Required: String |\n",
    "| ViolenceEvaluator | Required: String | Required: String | N/A | N/A |\n",
    "| SexualEvaluator | Required: String | Required: String | N/A | N/A |\n",
    "| SelfHarmEvaluator | Required: String | Required: String | N/A | N/A |\n",
    "| HateUnfairnessEvaluator | Required: String | Required: String | N/A | N/A |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from promptflow.entities import AzureOpenAIConnection, CognitiveSearchConnection\n",
    "\n",
    "model_connect = AzureOpenAIConnection(\n",
    "    name=os.environ.get(\"AISTUDIO_AOAI_CONNECTION_NAME\"),\n",
    "    api_base=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_type=\"azure\",\n",
    "    api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    #api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    ")\n",
    "\n",
    "ais_connect = CognitiveSearchConnection(\n",
    "    name=os.environ.get(\"AISTUDIO_AIS_CONNECTION_NAME\"),\n",
    "    api_base=os.environ.get(\"AZURE_AI_SEARCH_ENDPOINT\"),\n",
    "    api_key=os.environ.get(\"AZURE_AI_SEARCH_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "myflow = \"../3-metaprompt-grounding/prompt-flow/product-chat\"\n",
    "myflow = \"../3-metaprompt-grounding/copilot-flow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PromptyModelConfiguration.__init__() missing 1 required positional argument: 'parameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m flow_path \u001b[38;5;241m=\u001b[39m myflow\n\u001b[1;32m      5\u001b[0m sample_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/evaluation_dataset.jsonl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;66;03m# data to be evaluated\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[43mload_flow\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmyflow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchat.prompty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m f\u001b[38;5;241m.\u001b[39mcontext\u001b[38;5;241m.\u001b[39mconnections \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetermineIntent\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconnection\u001b[39m\u001b[38;5;124m\"\u001b[39m: model_connect}, \n\u001b[1;32m     10\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrieveDocuments\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msearchConnection\u001b[39m\u001b[38;5;124m\"\u001b[39m: ais_connect, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddingModelConnection\u001b[39m\u001b[38;5;124m\"\u001b[39m: model_connect}, \n\u001b[1;32m     11\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetermineReply\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconnection\u001b[39m\u001b[38;5;124m\"\u001b[39m: model_connect}}\n\u001b[1;32m     13\u001b[0m result \u001b[38;5;241m=\u001b[39m f(url\u001b[38;5;241m=\u001b[39msample_input)\n",
      "File \u001b[0;32m~/miniconda3/envs/pf-eval/lib/python3.12/site-packages/promptflow/_sdk/_load_functions.py:84\u001b[0m, in \u001b[0;36mload_flow\u001b[0;34m(source, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_flow\u001b[39m(\n\u001b[1;32m     72\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, PathLike, IO[AnyStr]],\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     74\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Flow:\n\u001b[1;32m     75\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load flow from YAML file.\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03m    :param source: The local yaml source of a flow. Must be a path to a local file.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m    :rtype: ~promptflow._sdk.entities._flows.Flow\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFlow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pf-eval/lib/python3.12/site-packages/promptflow/_sdk/entities/_flows/base.py:209\u001b[0m, in \u001b[0;36mFlow.load\u001b[0;34m(cls, source, raise_error, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03mLoad flow from YAML file.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m:rtype: Flow\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    208\u001b[0m _, flow_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_load_prepare(source)\n\u001b[0;32m--> 209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch_flow_creation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflow_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraise_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pf-eval/lib/python3.12/site-packages/promptflow/_sdk/entities/_flows/dag.py:144\u001b[0m, in \u001b[0;36mFlow._dispatch_flow_creation\u001b[0;34m(cls, flow_path, raise_error, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpromptflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mentities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_flows\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FlexFlow, Prompty\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_prompty_flow(file_path\u001b[38;5;241m=\u001b[39mflow_path, raise_error\u001b[38;5;241m=\u001b[39mraise_error):\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPrompty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflow_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(flow_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39mDEFAULT_ENCODING) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    147\u001b[0m     flow_content \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/miniconda3/envs/pf-eval/lib/python3.12/site-packages/promptflow/_sdk/entities/_flows/prompty.py:50\u001b[0m, in \u001b[0;36mPrompty._load\u001b[0;34m(cls, path, raise_error, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load\u001b[39m(\u001b[38;5;28mcls\u001b[39m, path: Path, raise_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 50\u001b[0m     core_prompty \u001b[38;5;241m=\u001b[39m \u001b[43mCorePrompty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# raise validation error on unknown fields\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raise_error:\n",
      "File \u001b[0;32m~/miniconda3/envs/pf-eval/lib/python3.12/site-packages/promptflow/core/_flow.py:323\u001b[0m, in \u001b[0;36mPrompty.__init__\u001b[0;34m(self, path, model, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m configs \u001b[38;5;241m=\u001b[39m update_dict_recursively(configs, resolve_references(kwargs, base_path\u001b[38;5;241m=\u001b[39mpath\u001b[38;5;241m.\u001b[39mparent))\n\u001b[1;32m    319\u001b[0m configs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m update_dict_recursively(\n\u001b[1;32m    320\u001b[0m     configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}), resolve_references(model \u001b[38;5;129;01mor\u001b[39;00m {}, base_path\u001b[38;5;241m=\u001b[39mpath\u001b[38;5;241m.\u001b[39mparent)\n\u001b[1;32m    321\u001b[0m )\n\u001b[0;32m--> 323\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m \u001b[43mPromptyModelConfiguration\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfigs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inputs \u001b[38;5;241m=\u001b[39m configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs \u001b[38;5;241m=\u001b[39m configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n",
      "\u001b[0;31mTypeError\u001b[0m: PromptyModelConfiguration.__init__() missing 1 required positional argument: 'parameters'"
     ]
    }
   ],
   "source": [
    "from promptflow.client import load_flow\n",
    "\n",
    "\n",
    "flow_path = myflow\n",
    "sample_input = '../data/evaluation_dataset.jsonl', # data to be evaluated\n",
    "\n",
    "f = load_flow(source=os.path.join(myflow, \"chat.prompty\"))\n",
    "\n",
    "f.context.connections = {\"DetermineIntent\": {\"connection\": model_connect}, \n",
    "                         \"RetrieveDocuments\": {\"searchConnection\": ais_connect, \"embeddingModelConnection\": model_connect}, \n",
    "                         \"DetermineReply\": {\"connection\": model_connect}}\n",
    "\n",
    "result = f(url=sample_input)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential does not work\n",
    "    credential = InteractiveBrowserCredential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: /home/krbock/GitHub/rai-genai-workshop/config.json\n"
     ]
    }
   ],
   "source": [
    "from promptflow.azure import PFClient\n",
    "\n",
    "# Connect to the workspace\n",
    "pf = PFClient.from_config(credential=credential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = \"../3-metaprompt-grounding/copilot-flow\"  # path to the flow directory\n",
    "data = \"./data.jsonl\"  # path to the data file\n",
    "\n",
    "# create run with the flow and data\n",
    "base_run = pf.run(\n",
    "    flow=flow,\n",
    "    init={\n",
    "        \"model_config\": config,\n",
    "    },\n",
    "    data=data,\n",
    "    column_mapping={\n",
    "        \"question\": \"${data.question}\",\n",
    "        \"chat_history\": \"${data.chat_history}\",\n",
    "    },\n",
    "    stream=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_flow = \"../3-metaprompt-grounding/copilot-flow\"\n",
    "config = AzureOpenAIModelConfiguration(\n",
    "    connection=\"open_ai_connection\", azure_deployment=\"gpt-4o\"\n",
    ")\n",
    "eval_run = pf.run(\n",
    "    flow=eval_flow,\n",
    "    init={\n",
    "        \"model_config\": config,\n",
    "    },\n",
    "    data=\"./data.jsonl\",  # path to the data file\n",
    "    run=base_run,  # specify base_run as the run you want to evaluate\n",
    "    column_mapping={\n",
    "        \"answer\": \"${run.outputs.output}\",\n",
    "        \"statements\": \"${data.statements}\",\n",
    "    },\n",
    "    stream=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## Test using a flow\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m( \n\u001b[1;32m      3\u001b[0m     evaluation_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqa-eval-with-flow\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m#name your evaluation to view in AI Studio\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     target\u001b[38;5;241m=\u001b[39mllm_tool, \u001b[38;5;66;03m# pass in a flow that you want to run then evaluate results on \u001b[39;00m\n\u001b[1;32m      5\u001b[0m     data\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/evaluation_dataset.jsonl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;66;03m# data to be evaluated\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     task_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqa\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# for different task types, different metrics are available\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     metrics_list\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt_groundedness\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt_relevance\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt_coherence\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt_fluency\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt_similarity\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;66;03m#optional superset over default set of metrics\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# model_config= { #for AI-assisted metrics, need to hook up AOAI GPT model for doing the measurement\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m#         \"api_version\": \"2023-05-15\",\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m#         \"api_base\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m#         \"api_type\": \"azure\",\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m#         \"api_key\": os.getenv(\"AZURE_OPENAI_KEY\"),\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m#         \"deployment_id\": os.getenv(\"AZURE_OPENAI_EVALUATION_DEPLOYMENT\")\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# },\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     data_mapping\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestions\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m#column of data providing input to model\u001b[39;00m\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontexts\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m#column of data providing context for each input\u001b[39;00m\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m#column of data providing output from model\u001b[39;00m\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_test\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroundtruth\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m#column of data providing ground truth answer, optional for default metrics\u001b[39;00m\n\u001b[1;32m     20\u001b[0m         },\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Optionally provide your AI Studio project information to track your evaluation results in your Azure AI studio project\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     azure_ai_project \u001b[38;5;241m=\u001b[39m azure_ai_project,\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# Optionally provide an output path to dump a json of metric summary, row level data and metric and studio URL\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     output_path\u001b[38;5;241m=\u001b[39moutput_dir\n\u001b[1;32m     25\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluate' is not defined"
     ]
    }
   ],
   "source": [
    "## Test using a flow\n",
    "result = evaluate( \n",
    "    evaluation_name=\"qa-eval-with-flow\", #name your evaluation to view in AI Studio\n",
    "    target=llm_tool, # pass in a flow that you want to run then evaluate results on \n",
    "    data='../data/evaluation_dataset.jsonl', # data to be evaluated\n",
    "    task_type=\"qa\", # for different task types, different metrics are available\n",
    "    metrics_list=[\"gpt_groundedness\", \"gpt_relevance\", \"gpt_coherence\", \"gpt_fluency\", \"gpt_similarity\"], #optional superset over default set of metrics\n",
    "    # model_config= { #for AI-assisted metrics, need to hook up AOAI GPT model for doing the measurement\n",
    "    #         \"api_version\": \"2023-05-15\",\n",
    "    #         \"api_base\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    #         \"api_type\": \"azure\",\n",
    "    #         \"api_key\": os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    #         \"deployment_id\": os.getenv(\"AZURE_OPENAI_EVALUATION_DEPLOYMENT\")\n",
    "    # },\n",
    "    data_mapping={\n",
    "        \"questions\":\"question\", #column of data providing input to model\n",
    "        \"contexts\":\"context\", #column of data providing context for each input\n",
    "        \"y_pred\":\"answer\", #column of data providing output from model\n",
    "        \"y_test\":\"groundtruth\" #column of data providing ground truth answer, optional for default metrics\n",
    "        },\n",
    "    # Optionally provide your AI Studio project information to track your evaluation results in your Azure AI studio project\n",
    "    azure_ai_project = azure_ai_project,\n",
    "    # Optionally provide an output path to dump a json of metric summary, row level data and metric and studio URL\n",
    "    output_path=output_dir\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = evaluate(\n",
    "    target=copilot_wrapper,\n",
    "    evaluation_name=\"qa-eval-with-flow\", #name your evaluation to view in AI Studio\n",
    "    data='../data/evaluation_dataset.jsonl', # data to be evaluated\n",
    "    evaluators={\n",
    "        \"relevance\": relevance_eval,\n",
    "        \"groundedness\": groundedness_eval,\n",
    "        \"coherence\": coherence_eval,\n",
    "        \"fluency\": fluency_eval,\n",
    "        \"similarity\": similarity_eval,\n",
    "        \"f1score\": f1score_eval\n",
    "    },\n",
    "    evaluator_config={\n",
    "        \"relevance\": {\"question\": \"${data.question}\"},\n",
    "        \"coherence\": {\"question\": \"${data.question}\"},\n",
    "        \"groundedness\": {\"question\": \"${data.question}\"},\n",
    "        \"fluency\": {\"question\": \"${data.answer}\"},\n",
    "        \n",
    "                \"default\": {\n",
    "            \"questions\": \"${data.question)\", #column of data providing input to model\n",
    "            #\"contexts\": \"${data.context}\", #column of data providing context for each input\n",
    "            \"answer\": \"${target.answer}\", #column of data providing output from model\n",
    "            \"ground_truth\":\"${data.truth}\" #column of data providing ground truth answer, optional for default metrics\n",
    "        }\n",
    "    },\n",
    "    # to log evaluation to the cloud AI Studio project\n",
    "    azure_ai_project={\n",
    "        \"subscription_id\": os.getenv(\"AZURE_SUBSCRIPTION_ID\"),\n",
    "        \"resource_group_name\": os.getenv(\"AZURE_RESOURCE_GROUP\"),\n",
    "        \"project_name\": os.getenv(\"AZUREAI_PROJECT_NAME\"),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "target must be a callable function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## Test using a flow\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mqa-eval-with-flow\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#name your evaluation to view in AI Studio\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmyflow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# pass in a flow that you want to run then evaluate results on \u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmydata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# data to be evaluated\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mqa\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# for different task types, different metrics are available\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt_groundedness\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt_relevance\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt_coherence\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt_fluency\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt_similarity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#optional superset over default set of metrics\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#for AI-assisted metrics, need to hook up AOAI GPT model for doing the measurement\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapi_version\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2023-05-15\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapi_base\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAZURE_OPENAI_ENDPOINT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapi_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mazure\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapi_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAZURE_OPENAI_KEY\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdeployment_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAZURE_OPENAI_EVALUATION_DEPLOYMENT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#column of data providing input to model\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontexts\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#column of data providing context for each input\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my_pred\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43manswer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#column of data providing output from model\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my_test\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgroundtruth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#column of data providing ground truth answer, optional for default metrics\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Optionally provide your AI Studio project information to track your evaluation results in your Azure AI studio project\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mazure_ai_project\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mazure_ai_project\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Optionally provide an output path to dump a json of metric summary, row level data and metric and studio URL\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dir\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pf-eval/lib/python3.12/site-packages/promptflow/evals/evaluate/_evaluate.py:331\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(evaluation_name, target, data, evaluators, evaluator_config, azure_ai_project, output_path, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Evaluates target or data with built-in or custom evaluators. If both target and data are provided,\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;124;03m    data will be run through target function and then results will be evaluated.\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    326\u001b[0m \n\u001b[1;32m    327\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    329\u001b[0m trace_destination \u001b[38;5;241m=\u001b[39m _trace_destination_from_project_scope(azure_ai_project) \u001b[38;5;28;01mif\u001b[39;00m azure_ai_project \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 331\u001b[0m input_data_df \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_and_load_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mazure_ai_project\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluation_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;66;03m# Process evaluator config to replace ${target.} with ${data.}\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluator_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/pf-eval/lib/python3.12/site-packages/promptflow/evals/evaluate/_evaluate.py:90\u001b[0m, in \u001b[0;36m_validate_and_load_data\u001b[0;34m(target, data, evaluators, output_path, azure_ai_project, evaluation_name)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(target):\n\u001b[0;32m---> 90\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget must be a callable function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[0;31mValueError\u001b[0m: target must be a callable function."
     ]
    }
   ],
   "source": [
    "## Test using a flow\n",
    "result = evaluate( \n",
    "    evaluation_name=\"qa-eval-with-flow\", #name your evaluation to view in AI Studio\n",
    "    target=myflow, # pass in a flow that you want to run then evaluate results on \n",
    "    data=mydata, # data to be evaluated\n",
    "    task_type=\"qa\", # for different task types, different metrics are available\n",
    "    metrics_list=[\"gpt_groundedness\", \"gpt_relevance\", \"gpt_coherence\", \"gpt_fluency\", \"gpt_similarity\"], #optional superset over default set of metrics\n",
    "    model_config= { #for AI-assisted metrics, need to hook up AOAI GPT model for doing the measurement\n",
    "            \"api_version\": \"2023-05-15\",\n",
    "            \"api_base\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "            \"api_type\": \"azure\",\n",
    "            \"api_key\": os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "            \"deployment_id\": os.getenv(\"AZURE_OPENAI_EVALUATION_DEPLOYMENT\")\n",
    "    },\n",
    "    data_mapping={\n",
    "        \"questions\":\"question\", #column of data providing input to model\n",
    "        \"contexts\":\"context\", #column of data providing context for each input\n",
    "        \"y_pred\":\"answer\", #column of data providing output from model\n",
    "        \"y_test\":\"groundtruth\" #column of data providing ground truth answer, optional for default metrics\n",
    "        },\n",
    "    # Optionally provide your AI Studio project information to track your evaluation results in your Azure AI studio project\n",
    "    azure_ai_project = azure_ai_project,\n",
    "    # Optionally provide an output path to dump a json of metric summary, row level data and metric and studio URL\n",
    "    output_path=output_dir\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'azure.ai.generative'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#pip install azure-ai-generative[simulator]\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mazure\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerative\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msynthetic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimulator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Simulator\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'azure.ai.generative'"
     ]
    }
   ],
   "source": [
    "#pip install azure-ai-generative[simulator]\n",
    "from azure.ai.generative.synthetic.simulator import Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.evals.synthetic import AdversarialSimulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def callback(\n",
    "    messages: List[Dict],\n",
    "    stream: bool = False,\n",
    "    session_state: Any = None,\n",
    ") -> dict:\n",
    "    query = messages[\"messages\"][0][\"content\"]\n",
    "    context = None\n",
    "\n",
    "    # Add file contents for summarization or re-write\n",
    "    if 'file_content' in messages[\"template_parameters\"]:\n",
    "        query += messages[\"template_parameters\"]['file_content']\n",
    "    \n",
    "    # Call your own endpoint and pass your query as input. Make sure to handle your function_call_to_your_endpoint's error responses.\n",
    "    response = await function_call_to_your_endpoint(query) \n",
    "    \n",
    "    # Format responses in OpenAI message protocol\n",
    "    formatted_response = {\n",
    "        \"content\": response,\n",
    "        \"role\": \"assistant\",\n",
    "        \"context\": {},\n",
    "    }\n",
    "\n",
    "    messages[\"messages\"].append(formatted_response)\n",
    "    return {\n",
    "        \"messages\": messages[\"messages\"],\n",
    "        \"stream\": stream,\n",
    "        \"session_state\": session_state\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.evals.synthetic import AdversarialScenario\n",
    "\n",
    "scenario = AdversarialScenario.ADVERSARIAL_QA\n",
    "simulator = AdversarialSimulator(azure_ai_project=azure_ai_project)\n",
    "\n",
    "outputs = await simulator(\n",
    "        scenario=scenario, # required adversarial scenario to simulate\n",
    "        target=callback, # callback function to simulate against\n",
    "        max_conversation_turns=1, #optional, applicable only to conversation scenario\n",
    "        max_simulation_results=3, #optional\n",
    "        jailbreak=False #optional\n",
    "    )\n",
    "\n",
    "# By default simulator outputs json, use the following helper function to convert to QA pairs in jsonl format\n",
    "print(outputs.to_eval_qa_json_lines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.resources.client import AIClient\n",
    "from azure.ai.resources.entities import AzureOpenAIModelConfiguration\n",
    "\n",
    "# initialize ai_client. This assums that config.json downloaded from ai workspace is present in the working directory\n",
    "ai_client = AIClient.from_config(DefaultAzureCredential())\n",
    "# Retrieve default aoai connection if it exists\n",
    "aoai_connection = ai_client.get_default_aoai_connection()\n",
    "# alternatively, retrieve connection by name\n",
    "# aoai_connection = ai_client.connections.get(\"<name of connection>\")\n",
    "\n",
    "# # Specify model and deployment name for your system large language model\n",
    "# aoai_config = AzureOpenAIModelConfiguration.from_connection(\n",
    "#     connection=aoai_connection,\n",
    "#     model_name=os.getenv('AZURE_OPENAI_EVALUATION_MODEL'),\n",
    "#     deployment_name=os.getenv('AZURE_OPENAI_EVALUATION_DEPLOYMENT'),\n",
    "#     temperature=0.1,\n",
    "#     max_tokens=300\n",
    "# )\n",
    "# # Specify model and deployment name for your system large language model\n",
    "aoai_config = AzureOpenAIModelConfiguration.from_connection(\n",
    "    connection=aoai_connection,\n",
    "    model_name='gpt-4-32k',\n",
    "    deployment_name='gpt-4-32k-0613',\n",
    "    temperature=0.1,\n",
    "    max_tokens=300\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncAzureOpenAI\n",
    "oai_client = AsyncAzureOpenAI(api_key=os.getenv('AZURE_OPENAI_KEY'), azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'), api_version=\"2024-02-15-preview\")\n",
    "async_oai_chat_completion_fn = oai_client.chat.completions.create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_simulator = Simulator.from_fn(\n",
    "    fn=async_oai_chat_completion_fn, # Simulate against a local function OR callback function\n",
    "    simulator_connection=aoai_config # Configure the simulator\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = Simulator.get_template(\"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_params = [\n",
    "    {\n",
    "        \"name\": \"John Doe\",\n",
    "        \"chatbot_name\": \"AI Chatbot\",\n",
    "        \"filename\": \"company_report.txt\",\n",
    "        \"file_content\": \"The company is doing well. The stock price is up 10% this quarter. The company is expanding into new markets. The company is investing in new technology. The company is hiring new employees. The company is launching new products. The company is opening new stores. The company is increasing its market share. The company is increasing its revenue. The company is increasing its profits.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Jane Doe\",\n",
    "        \"chatbot_name\": \"AI Chatbot\",\n",
    "        \"filename\": \"sales_report.txt\",\n",
    "        \"file_content\": \"The sales team is doing well. The sales team is meeting its targets. The sales team is increasing its revenue. The sales team is increasing its market share. The sales team is increasing its profits. The sales team is expanding into new markets. The sales team is launching new products. The sales team is opening new stores. The sales team is hiring new employees. The sales team is investing in new technology.\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = await function_simulator.simulate_async(\n",
    "    template,\n",
    "    parameters=template_params,\n",
    "    max_conversation_turns=2,\n",
    "    api_call_delay_sec=10,\n",
    "    max_simulation_results=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate QA from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# product sample data\n",
    "texts_glob = Path(\"../data/product-info/\")\n",
    "# azureai-samples data\n",
    "#texts_glob = Path(\"../../azureai-samples/scenarios/generate-synthetic-data/ai-generated-data-qna/data/data_generator_texts/\")\n",
    "files = Path.glob(texts_glob, pattern=\"**/*\")\n",
    "files = [file for file in files if Path.is_file(file)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "import nltk\n",
    "\n",
    "# download pre-trained Punkt tokenizer for sentence splitting\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "text_splitter = NLTKTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=\"cl100k_base\",  # encoding for gpt-4 and gpt-35-turbo\n",
    "    chunk_size=300,  # number of tokens to split on\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "texts = []\n",
    "for file in files:\n",
    "    loader = UnstructuredFileLoader(file)\n",
    "    docs = loader.load()\n",
    "    data = docs[0].page_content\n",
    "    texts += text_splitter.split_text(data)\n",
    "print(f\"Number of texts after splitting: {len(texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.generative.synthetic.qa import QADataGenerator, QAType\n",
    "\n",
    "## Uses AzureOpenAI environment variables\n",
    "\n",
    "# For granular logs you may set DEBUG log level:\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.DEBUG)\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "model_config = {\n",
    "    \"deployment\": \"gpt-4-1106-preview\",\n",
    "    \"model\": \"gpt-4\",\n",
    "    \"max_tokens\": 2000,\n",
    "}\n",
    "\n",
    "qa_generator = QADataGenerator(model_config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QADataGenerator(model_config=model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate QA asynchronously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.generative.synthetic.qa import QADataGenerator, QAType\n",
    "import asyncio\n",
    "from collections import Counter\n",
    "from typing import Dict\n",
    "\n",
    "concurrency = 3  # number of concurrent calls\n",
    "sem = asyncio.Semaphore(concurrency)\n",
    "\n",
    "qa_type = QAType.CONVERSATION\n",
    "\n",
    "\n",
    "async def generate_async(text: str) -> Dict:\n",
    "    async with sem:\n",
    "        return await qa_generator.generate_async(\n",
    "            text=text,\n",
    "            qa_type=qa_type,\n",
    "            num_questions=3,  # Number of questions to generate per text\n",
    "        )\n",
    "\n",
    "\n",
    "results = await asyncio.gather(*[generate_async(text) for text in texts], return_exceptions=True)\n",
    "\n",
    "question_answer_list = []\n",
    "token_usage = Counter()\n",
    "for result in results:\n",
    "    if isinstance(result, Exception):\n",
    "        raise result  # exception raised inside generate_async()\n",
    "    question_answer_list.append(result[\"question_answers\"])\n",
    "    token_usage += result[\"token_usage\"]\n",
    "\n",
    "print(\"Successfully generated QAs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Tokens used: {result['token_usage']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the generated data for later use\n",
    "Let us save the generated QnA in a format which can be understood by prompt flow (for evaluation, batch runs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "generated_dir = \"../data/generated_qa\"\n",
    "os.makedirs(generated_dir, exist_ok=True)\n",
    "output_file = os.path.join(generated_dir, \"generated_qa.jsonl\")\n",
    "qa_generator.export_to_file(output_file, qa_type, question_answer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-evaluation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
