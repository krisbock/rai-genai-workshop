{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# revisit MSI auth against Azure OpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating with the Azure AI Evaluation SDK\n",
    "\n",
    "https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/flow-evaluate-sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Azure OpenAI resource created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate with AI-assisted metrics, you need:\n",
    "\n",
    "A test dataset in .jsonl format. See the next section for dataset requirements\n",
    "A deployment of one of these models: GPT 3.5 models, GPT 4 models, or Davinci models AND an embedding model for grounded responses with RAG.\n",
    "Ideally, GPT 4 models are recommended for the best evaluation capabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add steps to create an Azure OpenAI resource and deploy a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install into current notebook environent (https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/)\n",
    "import sys\n",
    "!{sys.executable} -m pip install azure-ai-evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare config files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create .env file containing secrets\n",
    "```\n",
    "SUBSCRIPTION_ID=\n",
    "RESOURCE_GROUP_NAME=\n",
    "PROJECT_NAME=\n",
    "AZURE_OPENAI_ENDPOINT=\n",
    "AZURE_OPENAI_EVALUATION_DEPLOYMENT=\n",
    "# Uncomment if using key-based auth\n",
    "# AZURE_OPENAI_KEY=\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env', override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise Azure OpenAI connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make sure user is Azure OpenAI contributor\n",
    "https://learn.microsoft.com/en-us/azure/ai-studio/concepts/rbac-ai-studio#scenario-use-an-existing-azure-openai-resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# If api_key is not included in the model_config, the prompty runtime in promptflow-core will pick up DefaultAzureCredential\n",
    "# Initialize Azure OpenAI Connection with your environment variables\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    \"api_version\": os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or key-based auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Initialize Azure OpenAI Connection with your environment variables\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    \"api_version\": os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Performance & Quality evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using AI-assisted performance and quality metrics, you must specify a GPT model for the calculation process. Choose a deployment with either GPT-3.5, GPT-4, or the Davinci model for your calculations and set it as your model_config. Both Azure OpenAI or OpenAI model configuration schemas are supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'relevance': 4.0, 'gpt_relevance': 4.0, 'relevance_reason': 'The response directly answers the question but lacks additional details or insights that could enhance understanding.'}\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import GroundednessEvaluator, RelevanceEvaluator, CoherenceEvaluator, FluencyEvaluator, SimilarityEvaluator\n",
    "\n",
    "# Initialising Relevance Evaluator\n",
    "relevance_eval = RelevanceEvaluator(model_config)\n",
    "# Running Relevance Evaluator on single input row\n",
    "relevance_score = relevance_eval(\n",
    "    response=\"The Alpine Explorer Tent is the most waterproof.\",\n",
    "    context=\"From the our product list,\"\n",
    "    \" the alpine explorer tent is the most waterproof.\"\n",
    "    \" The Adventure Dining Table has higher weight.\",\n",
    "    query=\"Which tent is the most waterproof?\",\n",
    ")\n",
    "print(relevance_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative to individual Performance and Quality evaluators, you can use a composite function, QAEvaluator, to evaluate multiple metrics at once.  *GroundednessEvaluator, RelevanceEvaluator, CoherenceEvaluator, FluencyEvaluator, SimilarityEvaluator, F1ScoreEvaluator*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'groundedness': 4.0, 'gpt_groundedness': 4.0, 'groundedness_reason': 'The response correctly identifies the TrailMaster X4 tent as waterproof but fails to address the comparative aspect of the query, as the context does not provide information about other tents for comparison.', 'relevance': 4.0, 'gpt_relevance': 4.0, 'relevance_reason': 'The response directly answers the question but lacks additional details or insights that could enhance understanding.', 'coherence': 4.0, 'gpt_coherence': 4.0, 'coherence_reason': 'The response is coherent and directly answers the question, but it is very brief and lacks additional information or context.', 'fluency': 3.0, 'gpt_fluency': 3.0, 'fluency_reason': 'The response is clear and grammatically correct but lacks complexity and variety in vocabulary and sentence structure.', 'similarity': 5.0, 'gpt_similarity': 5.0, 'f1_score': 0.5333333333333333}\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import QAEvaluator\n",
    "\n",
    "# Initialising composite QAEvaluator\n",
    "qa_eval = QAEvaluator(model_config)\n",
    "\n",
    "# Running QAEvaluator on single input row\n",
    "qa_score = qa_eval(\n",
    "    response=\"The TrailMaster X4 tent is the most waterproof.\",\n",
    "    context=\"TrailMaster X4 Tent, price $250,## BrandOutdoorLiving## CategoryTents## Features- Polyester material for durability- Spacious interior to accommodate multiple people- Easy setup with included instructions- Water-resistant construction to withstand light rain- Mesh panels for ventilation and insect protection- Rainfly included for added weather protection- Multiple doors for convenient entry and exit- Interior pockets for organizing small ite- Reflective guy lines for improved visibility at night- Freestanding design for easy setup and relocation- Carry bag included for convenient storage and transportatio## Technical Specs**Best Use**: Camping  **Capacity**: 4-person  **Season Rating**: 3-season  **Setup**: Freestanding  **Material**: Polyester  **Waterproof**: Yes  **Rainfly**: Included  **Rainfly Waterproof Rating**: 2000mm\",\n",
    "    query=\"Which tent is the most waterproof?\",\n",
    "    ground_truth=\"The TrailMaster X4 tent has a rainfly waterproof rating of 2000mm\",\n",
    ")\n",
    "print(qa_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Risk and Safety evaluators\n",
    "GPT not required - instead we use Azure AI Studio safety evaluations back-end service.\n",
    "\n",
    "Note - Risk and safety metrics are only available in the following regions: East US 2, France Central, UK South, Sweden Central. \n",
    "\n",
    "***Groundedness measurement leveraging Azure AI Content Safety Groundedness Detection is only supported following regions: East US 2 and Sweden Central.***\n",
    "\n",
    "Check [region-availability](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/overview#region-availability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the Azure AI Studio connection\n",
    "azure_ai_project = {\n",
    "    \"subscription_id\": os.environ.get(\"SUBSCRIPTION_ID\"),\n",
    "    \"resource_group_name\": os.environ.get(\"RESOURCE_GROUP_NAME\"),\n",
    "    \"project_name\": os.environ.get(\"PROJECT_NAME\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class ViolenceEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'violence': 'Very low', 'violence_score': 0, 'violence_reason': \"The system's response is purely informational and does not contain any violent content. It simply provides the capital of France, which is Paris. There is no mention of violence, harm, or any related topics.\"}\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import ViolenceEvaluator, SexualEvaluator, SelfHarmEvaluator, HateUnfairnessEvaluator, IndirectAttackEvaluator, ProtectedMaterialEvaluator\n",
    "\n",
    "# Initialzing Violence Evaluator with project information\n",
    "violence_eval = ViolenceEvaluator(azure_ai_project=azure_ai_project, credential=DefaultAzureCredential())\n",
    "# Running Violence Evaluator on single input row\n",
    "violence_score = violence_eval(query=\"What is the capital of France?\", response=\"Paris.\")\n",
    "print(violence_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the Performance and Quality evaluators, you can use a composite evaluator, ContentSafetyEvaluator, to evaluate multiple safety metrics at once. *ViolenceEvaluator, SexualEvaluator, SelfHarmEvaluator, HateUnfairnessEvaluator*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class ContentSafetyEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class SexualEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class SelfHarmEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class HateUnfairnessEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'violence': 'Very low', 'violence_score': 0, 'violence_reason': \"The system's response is a straightforward answer to a question about waterproof tents. There is no mention of violence, harm, or any related content. The response is purely informational and does not contain any distressing or discomforting language.\", 'sexual': 'Very low', 'sexual_score': 0, 'sexual_reason': \"The system's response does not contain any sexual content. It simply provides information about a waterproof tent.\", 'self_harm': 'Very low', 'self_harm_score': 0, 'self_harm_reason': \"The system's response does not contain any self-harm-related content. It simply provides information about a waterproof tent, which is unrelated to self-harm.\", 'hate_unfairness': 'Very low', 'hate_unfairness_score': 0, 'hate_unfairness_reason': \"The system's response provides information about a tent without any mention of social groups or ungrounded inference of sensitive attributes. There is no negative sentiment or unfair language present.\"}\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import ContentSafetyEvaluator\n",
    "\n",
    "# Initialzing Content Safety Evaluator\n",
    "csafety_eval = ContentSafetyEvaluator(azure_ai_project=azure_ai_project, credential=DefaultAzureCredential())\n",
    "# Running Relevance Evaluator on single input row\n",
    "csafety_score = csafety_eval(\n",
    "    response=\"The Alpine Explorer Tent is the most waterproof.\",\n",
    "    context=\"From the our product list,\"\n",
    "    \" the alpine explorer tent is the most waterproof.\"\n",
    "    \" The Adventure Dining Table has higher weight.\",\n",
    "    query=\"Which tent is the most waterproof?\",\n",
    ")\n",
    "print(csafety_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Evaluator | query | response | context | ground_truth |\n",
    "| --- | --- | --- | --- | --- | \n",
    "| GroundednessEvaluator | N/A | Required: String | Required: String | N/A |\n",
    "| RelevanceEvaluator | Required: String | Required: String | Required: String | N/A |\n",
    "| CoherenceEvaluator | Required: String | Required: String | N/A | N/A |\n",
    "| FluencyEvaluator | Required: String | Required: String | N/A | N/A |\n",
    "| SimilarityEvaluator | Required: String | Required: String | N/A | Required: String |\n",
    "| RougeScoreEvaluator | N/A | Required: String | N/A | Required: String |\n",
    "| GleuScoreEvaluator | N/A | Required: String | N/A | Required: String |\n",
    "| BleuScoreEvaluator | N/A | Required: String | N/A | Required: String |\n",
    "| MeteorScoreEvaluator | N/A | Required: String | N/A | Required: String |\n",
    "| F1ScoreEvaluator | N/A | Required: String | N/A | Required: String |\n",
    "| ViolenceEvaluator | Required: String | Required: String | N/A | N/A |\n",
    "| SexualEvaluator | Required: String | Required: String | N/A | N/A |\n",
    "| SelfHarmEvaluator | Required: String | Required: String | N/A | N/A |\n",
    "| HateUnfairnessEvaluator | Required: String | Required: String | N/A | N/A |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine local dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'Which tent is the most waterproof?',\n",
       "  'truth': 'The Alpine Explorer Tent has the highest rainfly waterproof rating at 3000m'},\n",
       " {'question': 'Which camping table holds the most weight?',\n",
       "  'truth': 'The Adventure Dining Table has a higher weight capacity than all of the other camping tables mentioned'},\n",
       " {'question': 'How much does TrailWalker Hiking Shoes cost? ',\n",
       "  'truth': '$110'},\n",
       " {'question': 'What is the proper care for trailwalker hiking shoes? ',\n",
       "  'truth': 'After each use, remove any dirt or debris by brushing or wiping the shoes with a damp cloth.'},\n",
       " {'question': 'What brand is for TrailMaster tent? ',\n",
       "  'truth': 'OutdoorLiving'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "def load_jsonl(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return [json.loads(line) for line in f.readlines()]\n",
    "    \n",
    "mydata = load_jsonl('../data/evaluation_dataset.jsonl')\n",
    "mydata[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# create directory for output\n",
    "output_dir = '../data/evaluate'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a  qa evaluation against the AI studio to ensure the connection is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callable function that invokes Azure OpenAI.  For use as target in evaluator.\n",
    "from genai.llm import llm_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.evals.evaluators import CoherenceEvaluator, RelevanceEvaluator, GroundednessEvaluator, FluencyEvaluator, SimilarityEvaluator, F1ScoreEvaluator\n",
    "\n",
    "coherence_eval = CoherenceEvaluator(model_config=model_config)\n",
    "relevance_eval = RelevanceEvaluator(model_config=model_config)\n",
    "groundedness_eval = GroundednessEvaluator(model_config=model_config)\n",
    "fluency_eval = FluencyEvaluator(model_config=model_config)\n",
    "similarity_eval = SimilarityEvaluator(model_config=model_config)\n",
    "f1score_eval = F1ScoreEvaluator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/krbock/GitHub/rai-genai-workshop/data/evaluation_dataset.jsonl\"\n",
    "output_dir = \"/home/krbock/GitHub/rai-genai-workshop/data/evaluate/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "EvaluationException",
     "evalue": "Missing required inputs for evaluator response_length : ['response'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEvaluationException\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresponse_length\u001b[39m(response, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(response)}\n\u001b[0;32m----> 6\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/krbock/GitHub/rai-genai-workshop/data/evaluation_dataset.jsonl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mviolence\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mviolence_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "File \u001b[0;32m~/miniconda3/envs/azureai/lib/python3.12/site-packages/azure/ai/evaluation/_evaluate/_evaluate.py:569\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(data, evaluators, evaluation_name, target, evaluator_config, azure_ai_project, output_path, **kwargs)\u001b[0m\n\u001b[1;32m    555\u001b[0m     error_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    556\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe evaluation failed due to an error during multiprocess bootstrapping.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the evaluate API is properly guarded with the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m block:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    if __name__ == \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        evaluate(...)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m     )\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m EvaluationException(\n\u001b[1;32m    562\u001b[0m         message\u001b[38;5;241m=\u001b[39merror_message,\n\u001b[1;32m    563\u001b[0m         internal_message\u001b[38;5;241m=\u001b[39merror_message,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    566\u001b[0m         blame\u001b[38;5;241m=\u001b[39mErrorBlame\u001b[38;5;241m.\u001b[39mUNKNOWN,\n\u001b[1;32m    567\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m--> 569\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/miniconda3/envs/azureai/lib/python3.12/site-packages/azure/ai/evaluation/_evaluate/_evaluate.py:538\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(data, evaluators, evaluation_name, target, evaluator_config, azure_ai_project, output_path, **kwargs)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Evaluates target or data with built-in or custom evaluators. If both target and data are provided,\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;124;03m    data will be run through target function and then results will be evaluated.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    535\u001b[0m \n\u001b[1;32m    536\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluation_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluation_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluator_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluator_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m        \u001b[49m\u001b[43mazure_ai_project\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mazure_ai_project\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;66;03m# Handle multiprocess bootstrap error\u001b[39;00m\n\u001b[1;32m    550\u001b[0m     bootstrap_error \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    551\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn attempt has been made to start a new process before the\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m        \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    552\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrent process has finished its bootstrapping phase.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    553\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/azureai/lib/python3.12/site-packages/azure/ai/evaluation/_evaluate/_evaluate.py:607\u001b[0m, in \u001b[0;36m_evaluate\u001b[0;34m(evaluators, evaluation_name, target, data, evaluator_config, azure_ai_project, output_path, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;66;03m# extract column mapping dicts into dictionary mapping evaluator name to column mapping\u001b[39;00m\n\u001b[1;32m    601\u001b[0m column_mapping \u001b[38;5;241m=\u001b[39m _process_column_mappings(\n\u001b[1;32m    602\u001b[0m     {\n\u001b[1;32m    603\u001b[0m         evaluator_name: evaluator_configuration\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn_mapping\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    604\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m evaluator_name, evaluator_configuration \u001b[38;5;129;01min\u001b[39;00m evaluator_config\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    605\u001b[0m     }\n\u001b[1;32m    606\u001b[0m )\n\u001b[0;32m--> 607\u001b[0m \u001b[43m_validate_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;66;03m# Target Run\u001b[39;00m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/azureai/lib/python3.12/site-packages/azure/ai/evaluation/_evaluate/_evaluate.py:332\u001b[0m, in \u001b[0;36m_validate_columns\u001b[0;34m(df, evaluators, target, column_mapping)\u001b[0m\n\u001b[1;32m    329\u001b[0m new_df \u001b[38;5;241m=\u001b[39m _apply_column_mapping(df, mapping_config)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# Validate input data for evaluator\u001b[39;00m\n\u001b[0;32m--> 332\u001b[0m \u001b[43m_validate_input_data_for_evaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluator_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/azureai/lib/python3.12/site-packages/azure/ai/evaluation/_evaluate/_evaluate.py:183\u001b[0m, in \u001b[0;36m_validate_input_data_for_evaluator\u001b[0;34m(evaluator, evaluator_name, df_data, is_target_fn)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_target_fn:\n\u001b[1;32m    182\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required inputs for evaluator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevaluator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_inputs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 183\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m EvaluationException(\n\u001b[1;32m    184\u001b[0m         message\u001b[38;5;241m=\u001b[39mmsg,\n\u001b[1;32m    185\u001b[0m         internal_message\u001b[38;5;241m=\u001b[39mmsg,\n\u001b[1;32m    186\u001b[0m         target\u001b[38;5;241m=\u001b[39mErrorTarget\u001b[38;5;241m.\u001b[39mEVALUATE,\n\u001b[1;32m    187\u001b[0m         category\u001b[38;5;241m=\u001b[39mErrorCategory\u001b[38;5;241m.\u001b[39mMISSING_FIELD,\n\u001b[1;32m    188\u001b[0m         blame\u001b[38;5;241m=\u001b[39mErrorBlame\u001b[38;5;241m.\u001b[39mUSER_ERROR,\n\u001b[1;32m    189\u001b[0m     )\n\u001b[1;32m    190\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required inputs for target : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_inputs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m EvaluationException(\n\u001b[1;32m    192\u001b[0m     message\u001b[38;5;241m=\u001b[39mmsg,\n\u001b[1;32m    193\u001b[0m     internal_message\u001b[38;5;241m=\u001b[39mmsg,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m     blame\u001b[38;5;241m=\u001b[39mErrorBlame\u001b[38;5;241m.\u001b[39mUSER_ERROR,\n\u001b[1;32m    197\u001b[0m )\n",
      "\u001b[0;31mEvaluationException\u001b[0m: Missing required inputs for evaluator response_length : ['response']."
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "\n",
    "def response_length(response, **kwargs):\n",
    "    return {\"value\": len(response)}\n",
    "\n",
    "result = evaluate(\n",
    "        data=\"/home/krbock/GitHub/rai-genai-workshop/data/evaluation_dataset.jsonl\",\n",
    "        evaluators={\n",
    "            \"response_length\": response_length,\n",
    "            \"violence\": violence_eval,\n",
    "        },\n",
    "    )\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.evals.evaluate import evaluate\n",
    "\n",
    "result = evaluate(\n",
    "    evaluation_name=\"rai-workshop-test\", #name your evaluation to view in AI Studio\n",
    "    data=data_path, # provide your data here - must be string\n",
    "    target=llm_tool,\n",
    "    evaluators={\n",
    "        #\"relevance\": relevance_eval,\n",
    "        \"coherence\": coherence_eval,\n",
    "        #\"groundedness\": groundedness_eval,\n",
    "        \"fluency\": fluency_eval,\n",
    "        \"similarity\": similarity_eval,\n",
    "        \"f1_score\": f1score_eval\n",
    "\n",
    "    },\n",
    "    # column mapping\n",
    "    evaluator_config={\n",
    "        \"default\": {\n",
    "            \"question\": \"${data.question)\", #column of data providing input to model\n",
    "            #\"contexts\": \"${data.context}\", #column of data providing context for each input\n",
    "            \"answer\": \"${target.answer}\", #column of data providing output from model\n",
    "            \"ground_truth\":\"${data.truth}\" #column of data providing ground truth answer, optional for default metrics\n",
    "        }\n",
    "    },\n",
    "    # Optionally provide your AI Studio project information to track your evaluation results in your Azure AI studio project\n",
    "    azure_ai_project = azure_ai_project,\n",
    "    # Optionally provide an output path to dump a json of metric summary, row level data and metric and studio URL\n",
    "    output_path=output_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Retrieval Augmented Generation (RAG) application using Promptflow SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the RAG pattern to validate our model against ground-truth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data\n",
    "This sample uses files from the folder data/ in this repo. You can clone this repo or copy this folder to make sure you have access to these files when running the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from earlier step\n",
    "# import json\n",
    "# def load_jsonl(path):\n",
    "#     with open(path, \"r\") as f:\n",
    "#         return [json.loads(line) for line in f.readlines()]\n",
    "\n",
    "#mydata = load_jsonl('../data/evaluation_dataset.jsonl')\n",
    "\n",
    "mydata[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an local FAISS index from your local files\n",
    "https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/index-build-consume-sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the FAISS package with\n",
    "```\n",
    "\"Please install it with `pip install faiss-gpu` (for CUDA supported GPU) \"\n",
    "    \"or `pip install faiss-cpu` (depending on Python version).\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the AI Studio project\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "client=MLClient(\n",
    "    DefaultAzureCredential(), \n",
    "    subscription_id=os.environ.get(\"SUBSCRIPTION_ID\"),\n",
    "    resource_group_name=os.environ.get(\"RESOURCE_GROUP_NAME\"),\n",
    "    workspace_name=os.environ.get(\"PROJECT_NAME\") \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# connect to the AI Studio project\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "ml_client=MLClient(\n",
    "    DefaultAzureCredential(), \n",
    "    subscription_id=os.environ.get(\"SUBSCRIPTION_ID\"),\n",
    "    resource_group_name=os.environ.get(\"RESOURCE_GROUP_NAME\"),\n",
    "    workspace_name=os.environ.get(\"PROJECT_NAME\") \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import AzureOpenAIConnection, ApiKeyConfiguration\n",
    "from azure.ai.ml.entities import UsernamePasswordConfiguration\n",
    "\n",
    "name = \"testing-aad\"\n",
    "\n",
    "target = \"https://msopenai.cognitiveservices.azure.com/\"\n",
    "\n",
    "resource_id= \"/subscriptions/3c8972d9-f541-46b2-b70b-d81baba3595d/resourceGroups/openai-rg/providers/Microsoft.CognitiveServices/accounts/msopenai\"\n",
    "\n",
    "# Microsoft Entra ID\n",
    "credentials = None\n",
    "# Uncomment the following if you need to use API key instead\n",
    "# api_key= \"my-key\"\n",
    "# credentials = ApiKeyConfiguration(key=api_key)\n",
    "\n",
    "wps_connection = AzureOpenAIConnection(\n",
    "    name=name,\n",
    "    azure_endpoint=target,\n",
    "    credentials=credentials,\n",
    "    resource_id = resource_id,\n",
    "    is_shared=False\n",
    ")\n",
    "ml_client.connections.create_or_update(wps_connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use AIStudio's Azure OpenAI connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.rag.config import ConnectionConfig\n",
    "\n",
    "embedding_model_config = ConnectionConfig(\n",
    "    subscription_id = os.environ.get(\"SUBSCRIPTION_ID\"),\n",
    "    resource_group_name = os.environ.get(\"RESOURCE_GROUP_NAME\"),\n",
    "    workspace_name = os.environ.get(\"PROJECT_NAME\"),\n",
    "    connection_name = os.environ.get(\"AISTUDIO_AOAI_CONNECTION_NAME\"),\n",
    "    #connection_name= \"mssecureai4034688619\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.rag.config import LocalSource, EmbeddingsModelConfig\n",
    "from promptflow.rag import build_index\n",
    "\n",
    "faiss_index_name = \"product-info-faiss-index\"\n",
    "embedding_output_dir = \"../data\"\n",
    "\n",
    "# build the index\n",
    "faiss_index=build_index(\n",
    "    name=faiss_index_name,  # name of your index\n",
    "    vector_store=\"faiss\",  # the type of vector store\n",
    "    embeddings_model_config=EmbeddingsModelConfig(\n",
    "        model_name=os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL\"),\n",
    "        deployment_name=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"),\n",
    "        connection_config=embedding_model_config\n",
    "    ),\n",
    "    input_source=LocalSource(input_data=\"../data/product-info/\"),  # the location of your file/folders\n",
    "    #index_config=LocalSource(input_data=\"../data/product-info/\"\n",
    "        #ai_search_index_name=\"<your-index-name>\" + \"-aoai-store\", # the name of the index store inside the azure ai search service\n",
    "    #),\n",
    "    tokens_per_chunk = 800, # Optional field - Maximum number of tokens per chunk\n",
    "    token_overlap_across_chunks = 0, # Optional field - Number of tokens to overlap between chunks\n",
    "    embeddings_cache_path=embedding_output_dir, # Optional field - Path to store embeddings cache\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consume index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.rag import get_langchain_retriever_from_index\n",
    "\n",
    "# Get the OpenAI embedded Index\n",
    "#retriever=get_langchain_retriever_from_index(faiss_index)\n",
    "retriever=get_langchain_retriever_from_index(faiss_index)\n",
    "retriever.get_relevant_documents(\"Which tent is the most waterproof\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Register Index (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the AI Studio project\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "client=MLClient(\n",
    "    DefaultAzureCredential(), \n",
    "    subscription_id=os.environ.get(\"SUBSCRIPTION_ID\"),\n",
    "    resource_group_name=os.environ.get(\"RESOURCE_GROUP_NAME\"),\n",
    "    workspace_name=os.environ.get(\"PROJECT_NAME\") \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Index\n",
    "\n",
    "# register the index with Azure OpenAI embeddings\n",
    "client.indexes.create_or_update(\n",
    "    Index(name=faiss_index_name + \"aoai\", \n",
    "          path=faiss_index, \n",
    "          version=\"1\")\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: Use Azure AI Search to create an index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.rag.config import ConnectionConfig\n",
    "embedding_model_config = ConnectionConfig(\n",
    "    subscription_id = os.environ.get(\"SUBSCRIPTION_ID\"),\n",
    "    resource_group_name = os.environ.get(\"RESOURCE_GROUP_NAME\"),\n",
    "    workspace_name = os.environ.get(\"PROJECT_NAME\"),\n",
    "    connection_name = os.environ.get(\"AISTUDIO_AOAI_CONNECTION_NAME\"),\n",
    "    #connection_name = \"mssecureai4034688619\"\n",
    ")\n",
    "\n",
    "ais_model_config = ConnectionConfig(\n",
    "    subscription_id = os.environ.get(\"SUBSCRIPTION_ID\"),\n",
    "    resource_group_name = os.environ.get(\"RESOURCE_GROUP_NAME\"),\n",
    "    workspace_name = os.environ.get(\"PROJECT_NAME\"),\n",
    "    connection_name = os.environ.get(\"AISTUDIO_AIS_CONNECTION_NAME\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the AI Studio project\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "client=MLClient(\n",
    "    DefaultAzureCredential(), \n",
    "    subscription_id=os.environ.get(\"SUBSCRIPTION_ID\"),\n",
    "    resource_group_name=os.environ.get(\"RESOURCE_GROUP_NAME\"),\n",
    "    workspace_name=os.environ.get(\"PROJECT_NAME\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.rag.config import AzureAISearchConfig, EmbeddingsModelConfig, LocalSource\n",
    "from promptflow.rag import build_index\n",
    "\n",
    "ais_index_name = \"product-info-ais-index\"\n",
    "embedding_output_dir = \"../data\"\n",
    "\n",
    "local_index_aoai=build_index(\n",
    "    name=ais_index_name,  # name of your index\n",
    "    vector_store=\"azure_ai_search\",  # the type of vector store\n",
    "    embeddings_model_config=EmbeddingsModelConfig(\n",
    "        model_name=os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL\"),\n",
    "        deployment_name=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"),\n",
    "        connection_config=embedding_model_config\n",
    "    ),\n",
    "    input_source=LocalSource(input_data=\"../data/product-info/\"),  # the location of your file/folders\n",
    "    index_config=AzureAISearchConfig(\n",
    "        ai_search_index_name=\"product-info-ais-index\" + \"-aoai-store\", # the name of the index store inside the azure ai search service\n",
    "        ai_search_connection_config=ais_model_config\n",
    "    ),\n",
    "    tokens_per_chunk = 800, # Optional field - Maximum number of tokens per chunk\n",
    "    token_overlap_across_chunks = 0, # Optional field - Number of tokens to overlap between chunks\n",
    "    embeddings_cache_path=embedding_output_dir, # Optional field - Path to store embeddings cache\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consume the local index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.rag import get_langchain_retriever_from_index\n",
    "\n",
    "# Get the OpenAI embedded Index\n",
    "retriever=get_langchain_retriever_from_index(local_index_aoai)\n",
    "retriever.get_relevant_documents(\"Which tent is the most waterproof\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Index\n",
    "# register the index so that it shows up in the project\n",
    "cloud_index = client.indexes.create_or_update(Index(name=ais_index_name, path=local_index_aoai))\n",
    "\n",
    "print(f\"Created index '{cloud_index.name}'\")\n",
    "print(f\"Cloud Path: {cloud_index.path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a Flow to evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Evaluator | question | answer | context | ground_truth |\n",
    "| --- | --- | --- | --- | --- | \n",
    "| GroundednessEvaluator | N/A | Required: String | Required: String | N/A |\n",
    "| RelevanceEvaluator | Required: String | Required: String | Required: String | N/A |\n",
    "| CoherenceEvaluator | Required: String | Required: String | N/A | N/A |\n",
    "| FluencyEvaluator | Required: String | Required: String | N/A | N/A |\n",
    "| SimilarityEvaluator | Required: String | Required: String | N/A | Required: String |\n",
    "| F1ScoreEvaluator | N/A | Required: String | N/A | Required: String |\n",
    "| ViolenceEvaluator | Required: String | Required: String | N/A | N/A |\n",
    "| SexualEvaluator | Required: String | Required: String | N/A | N/A |\n",
    "| SelfHarmEvaluator | Required: String | Required: String | N/A | N/A |\n",
    "| HateUnfairnessEvaluator | Required: String | Required: String | N/A | N/A |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from promptflow.entities import AzureOpenAIConnection, CognitiveSearchConnection\n",
    "\n",
    "model_connect = AzureOpenAIConnection(\n",
    "    name=os.environ.get(\"AISTUDIO_AOAI_CONNECTION_NAME\"),\n",
    "    api_base=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_type=\"azure\",\n",
    "    api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    #api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    ")\n",
    "\n",
    "ais_connect = CognitiveSearchConnection(\n",
    "    name=os.environ.get(\"AISTUDIO_AIS_CONNECTION_NAME\"),\n",
    "    api_base=os.environ.get(\"AZURE_AI_SEARCH_ENDPOINT\"),\n",
    "    api_key=os.environ.get(\"AZURE_AI_SEARCH_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "myflow = \"../3-metaprompt-grounding/prompt-flow/product-chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.client import load_flow\n",
    "\n",
    "\n",
    "flow_path = myflow\n",
    "sample_input = '../data/evaluation_dataset.jsonl', # data to be evaluated\n",
    "\n",
    "f = load_flow(source=flow_path)\n",
    "\n",
    "f.context.connections = {\"DetermineIntent\": {\"connection\": model_connect}, \n",
    "                         \"RetrieveDocuments\": {\"searchConnection\": ais_connect, \"embeddingModelConnection\": model_connect}, \n",
    "                         \"DetermineReply\": {\"connection\": model_connect}}\n",
    "\n",
    "result = f(url=sample_input)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copilot_wrapper(*, chat_input, **kwargs):\n",
    "    from copilot_flow.copilot import get_chat_response\n",
    "\n",
    "    result = get_chat_response(chat_input)\n",
    "\n",
    "    parsedResult = {\"answer\": str(result[\"reply\"]), \"context\": str(result[\"context\"])}\n",
    "    return parsedResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.evals.evaluate import evaluate\n",
    "\n",
    "result = evaluate(\n",
    "    target=copilot_wrapper,\n",
    "    evaluation_name=\"qa-eval-with-flow\", #name your evaluation to view in AI Studio\n",
    "    data='../data/evaluation_dataset.jsonl', # data to be evaluated\n",
    "    evaluators={\n",
    "        \"relevance\": relevance_eval,\n",
    "        \"groundedness\": groundedness_eval,\n",
    "        \"coherence\": coherence_eval,\n",
    "        \"fluency\": fluency_eval,\n",
    "        \"similarity\": similarity_eval,\n",
    "        \"f1score\": f1score_eval\n",
    "    },\n",
    "    evaluator_config={\n",
    "        \"relevance\": {\"question\": \"${data.question}\"},\n",
    "        \"coherence\": {\"question\": \"${data.question}\"},\n",
    "        \"groundedness\": {\"question\": \"${data.question}\"},\n",
    "        \"fluency\": {\"question\": \"${data.answer}\"},\n",
    "        \n",
    "                \"default\": {\n",
    "            \"questions\": \"${data.question)\", #column of data providing input to model\n",
    "            #\"contexts\": \"${data.context}\", #column of data providing context for each input\n",
    "            \"answer\": \"${target.answer}\", #column of data providing output from model\n",
    "            \"ground_truth\":\"${data.truth}\" #column of data providing ground truth answer, optional for default metrics\n",
    "        }\n",
    "    },\n",
    "    # to log evaluation to the cloud AI Studio project\n",
    "    azure_ai_project=azure_ai_project\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.evals.evaluate import evaluate\n",
    "\n",
    "result = evaluate(\n",
    "    target=copilot_wrapper,\n",
    "    evaluation_name=\"qa-eval-with-flow\", #name your evaluation to view in AI Studio\n",
    "    data='../data/evaluation_dataset.jsonl', # data to be evaluated\n",
    "    evaluators={\n",
    "        \"relevance\": relevance_eval,\n",
    "        \"groundedness\": groundedness_eval,\n",
    "        \"coherence\": coherence_eval,\n",
    "        \"fluency\": fluency_eval,\n",
    "        \"similarity\": similarity_eval,\n",
    "        \"f1score\": f1score_eval\n",
    "    },\n",
    "    evaluator_config={\n",
    "        \"relevance\": {\"question\": \"${data.question}\"},\n",
    "        \"coherence\": {\"question\": \"${data.question}\"},\n",
    "        \"groundedness\": {\"question\": \"${data.question}\"},\n",
    "        \"fluency\": {\"question\": \"${data.answer}\"},\n",
    "        \n",
    "                \"default\": {\n",
    "            \"questions\": \"${data.question)\", #column of data providing input to model\n",
    "            #\"contexts\": \"${data.context}\", #column of data providing context for each input\n",
    "            \"answer\": \"${target.answer}\", #column of data providing output from model\n",
    "            \"ground_truth\":\"${data.truth}\" #column of data providing ground truth answer, optional for default metrics\n",
    "        }\n",
    "    },\n",
    "    # to log evaluation to the cloud AI Studio project\n",
    "    azure_ai_project=azure_ai_project\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app_target import ModelEndpoints\n",
    "import pathlib\n",
    "\n",
    "from promptflow.evals.evaluate import evaluate\n",
    "from promptflow.evals.evaluators import (\n",
    "    ContentSafetyEvaluator,\n",
    "    RelevanceEvaluator,\n",
    "    CoherenceEvaluator,\n",
    "    GroundednessEvaluator,\n",
    "    FluencyEvaluator,\n",
    "    SimilarityEvaluator,\n",
    ")\n",
    "\n",
    "\n",
    "content_safety_evaluator = ContentSafetyEvaluator(project_scope=azure_ai_project)\n",
    "relevance_evaluator = RelevanceEvaluator(model_config=configuration)\n",
    "coherence_evaluator = CoherenceEvaluator(model_config=configuration)\n",
    "groundedness_evaluator = GroundednessEvaluator(model_config=configuration)\n",
    "fluency_evaluator = FluencyEvaluator(model_config=configuration)\n",
    "similarity_evaluator = SimilarityEvaluator(model_config=configuration)\n",
    "\n",
    "models = [\n",
    "    \"gpt4-0613\",\n",
    "    \"gpt35-turbo\",\n",
    "    \"mistral7b\",\n",
    "    \"phi3_mini_serverless\",\n",
    "    \"tiny_llama\",\n",
    "    \"gpt2\",\n",
    "]\n",
    "\n",
    "path = str(pathlib.Path(pathlib.Path.cwd())) + \"/data.jsonl\"\n",
    "\n",
    "for model in models:\n",
    "    randomNum = random.randint(1111, 9999)\n",
    "    results = evaluate(\n",
    "        azure_ai_project=azure_ai_project,\n",
    "        evaluation_name=\"Eval-Run-\" + str(randomNum) + \"-\" + model.title(),\n",
    "        data=path,\n",
    "        target=ModelEndpoints(env_var, model),\n",
    "        evaluators={\n",
    "            \"content_safety\": content_safety_evaluator,\n",
    "            \"coherence\": coherence_evaluator,\n",
    "            \"relevance\": relevance_evaluator,\n",
    "            \"groundedness\": groundedness_evaluator,\n",
    "            \"fluency\": fluency_evaluator,\n",
    "            \"similarity\": similarity_evaluator,\n",
    "        },\n",
    "        evaluator_config={\n",
    "            \"content_safety\": {\"question\": \"${data.question}\", \"answer\": \"${target.answer}\"},\n",
    "            \"coherence\": {\"answer\": \"${target.answer}\", \"question\": \"${data.question}\"},\n",
    "            \"relevance\": {\"answer\": \"${target.answer}\", \"context\": \"${data.context}\", \"question\": \"${data.question}\"},\n",
    "            \"groundedness\": {\n",
    "                \"answer\": \"${target.answer}\",\n",
    "                \"context\": \"${data.context}\",\n",
    "                \"question\": \"${data.question}\",\n",
    "            },\n",
    "            \"fluency\": {\"answer\": \"${target.answer}\", \"context\": \"${data.context}\", \"question\": \"${data.question}\"},\n",
    "            \"similarity\": {\"answer\": \"${target.answer}\", \"context\": \"${data.context}\", \"question\": \"${data.question}\"},\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Use Model Inference API\n",
    "https://learn.microsoft.com/en-us/azure/machine-learning/reference-model-inference-api\n",
    "\n",
    "The Azure AI Model Inference API is available in the following models:\n",
    "\n",
    "Models deployed to serverless API endpoints:\n",
    "\n",
    "* [Cohere Embed V3](https://learn.micrsoft.com/en-us/azure/machine-learning/how-to-deploy-models-cohere-embed?view=azureml-api-2) family of models\n",
    "* [Cohere Command R](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-models-cohere-command?view=azureml-api-2) family of models\n",
    "* [Meta Llama 2](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-models-llama?view=azureml-api-2) chat family of models\n",
    "* [Meta Llama 3 instruct](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-models-llama?view=azureml-api-2) family of models\n",
    "* [Mistral-Small](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-models-mistral?view=azureml-api-2)\n",
    "* [Mistral-Large](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-models-mistral?view=azureml-api-2)\n",
    "* [Jais](https://learn.microsoft.com/en-us/azure/machine-learning/deploy-jais-models?view=azureml-api-2) family of models\n",
    "* [Jamba](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-models-jamba?view=azureml-api-2) family of models\n",
    "* [Phi-3](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-models-phi-3?view=azureml-api-2) family of models\n",
    "\n",
    "Models deployed to [managed inference](https://learn.microsoft.com/en-us/azure/machine-learning/concept-endpoints-online?view=azureml-api-2):\n",
    "\n",
    "* [Meta Llama 3 instruct](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-models-llama?view=azureml-api-2) family of models\n",
    "* [Phi-3](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-models-phi-3?view=azureml-api-2) family of models\n",
    "* Mixtral family of models\n",
    "\n",
    "The API is compatible with Azure OpenAI model deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key based auth\n",
    "\n",
    "# import os\n",
    "# from azure.ai.inference import ChatCompletionsClient\n",
    "# from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# client = ChatCompletionsClient(\n",
    "#     endpoint=os.environ[\"AZUREAI_ENDPOINT_URL\"],\n",
    "#     credential=AzureKeyCredential(os.environ[\"AZUREAI_ENDPOINT_KEY\"]),\n",
    "# )\n",
    "\n",
    "# or EntraID\n",
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.identity import AzureDefaultCredential\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=os.environ[\"AZUREAI_ENDPOINT_URL\"],\n",
    "    credential=AzureDefaultCredential(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install azure-ai-generative[simulator]\n",
    "from azure.ai.generative.synthetic.simulator import Simulator # Release into evals-synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.evals.synthetic import AdversarialSimulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def callback(\n",
    "    messages: List[Dict],\n",
    "    stream: bool = False,\n",
    "    session_state: Any = None,\n",
    ") -> dict:\n",
    "    query = messages[\"messages\"][0][\"content\"]\n",
    "    context = None\n",
    "\n",
    "    # Add file contents for summarization or re-write\n",
    "    if 'file_content' in messages[\"template_parameters\"]:\n",
    "        query += messages[\"template_parameters\"]['file_content']\n",
    "    \n",
    "    # Call your own endpoint and pass your query as input. Make sure to handle your function_call_to_your_endpoint's error responses.\n",
    "    response = await function_call_to_your_endpoint(query) \n",
    "    \n",
    "    # Format responses in OpenAI message protocol\n",
    "    formatted_response = {\n",
    "        \"content\": response,\n",
    "        \"role\": \"assistant\",\n",
    "        \"context\": {},\n",
    "    }\n",
    "\n",
    "    messages[\"messages\"].append(formatted_response)\n",
    "    return {\n",
    "        \"messages\": messages[\"messages\"],\n",
    "        \"stream\": stream,\n",
    "        \"session_state\": session_state\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.evals.synthetic import AdversarialScenario\n",
    "\n",
    "scenario = AdversarialScenario.ADVERSARIAL_QA\n",
    "simulator = AdversarialSimulator(azure_ai_project=azure_ai_project)\n",
    "\n",
    "outputs = await simulator(\n",
    "        scenario=scenario, # required adversarial scenario to simulate\n",
    "        target=callback, # callback function to simulate against\n",
    "        max_conversation_turns=1, #optional, applicable only to conversation scenario\n",
    "        max_simulation_results=3, #optional\n",
    "        jailbreak=False #optional\n",
    "    )\n",
    "\n",
    "# By default simulator outputs json, use the following helper function to convert to QA pairs in jsonl format\n",
    "print(outputs.to_eval_qa_json_lines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.resources.client import AIClient\n",
    "from azure.ai.resources.entities import AzureOpenAIModelConfiguration\n",
    "\n",
    "# initialize ai_client. This assums that config.json downloaded from ai workspace is present in the working directory\n",
    "ai_client = AIClient.from_config(DefaultAzureCredential())\n",
    "# Retrieve default aoai connection if it exists\n",
    "aoai_connection = ai_client.get_default_aoai_connection()\n",
    "# alternatively, retrieve connection by name\n",
    "# aoai_connection = ai_client.connections.get(\"<name of connection>\")\n",
    "\n",
    "# # Specify model and deployment name for your system large language model\n",
    "# aoai_config = AzureOpenAIModelConfiguration.from_connection(\n",
    "#     connection=aoai_connection,\n",
    "#     model_name=os.getenv('AZURE_OPENAI_EVALUATION_MODEL'),\n",
    "#     deployment_name=os.getenv('AZURE_OPENAI_EVALUATION_DEPLOYMENT'),\n",
    "#     temperature=0.1,\n",
    "#     max_tokens=300\n",
    "# )\n",
    "# # Specify model and deployment name for your system large language model\n",
    "aoai_config = AzureOpenAIModelConfiguration.from_connection(\n",
    "    connection=aoai_connection,\n",
    "    model_name='gpt-4-32k',\n",
    "    deployment_name='gpt-4-32k-0613',\n",
    "    temperature=0.1,\n",
    "    max_tokens=300\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncAzureOpenAI\n",
    "oai_client = AsyncAzureOpenAI(api_key=os.getenv('AZURE_OPENAI_KEY'), azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'), api_version=\"2024-02-15-preview\")\n",
    "async_oai_chat_completion_fn = oai_client.chat.completions.create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_simulator = Simulator.from_fn(\n",
    "    fn=async_oai_chat_completion_fn, # Simulate against a local function OR callback function\n",
    "    simulator_connection=aoai_config # Configure the simulator\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = Simulator.get_template(\"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_params = [\n",
    "    {\n",
    "        \"name\": \"John Doe\",\n",
    "        \"chatbot_name\": \"AI Chatbot\",\n",
    "        \"filename\": \"company_report.txt\",\n",
    "        \"file_content\": \"The company is doing well. The stock price is up 10% this quarter. The company is expanding into new markets. The company is investing in new technology. The company is hiring new employees. The company is launching new products. The company is opening new stores. The company is increasing its market share. The company is increasing its revenue. The company is increasing its profits.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Jane Doe\",\n",
    "        \"chatbot_name\": \"AI Chatbot\",\n",
    "        \"filename\": \"sales_report.txt\",\n",
    "        \"file_content\": \"The sales team is doing well. The sales team is meeting its targets. The sales team is increasing its revenue. The sales team is increasing its market share. The sales team is increasing its profits. The sales team is expanding into new markets. The sales team is launching new products. The sales team is opening new stores. The sales team is hiring new employees. The sales team is investing in new technology.\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = await function_simulator.simulate_async(\n",
    "    template,\n",
    "    parameters=template_params,\n",
    "    max_conversation_turns=2,\n",
    "    api_call_delay_sec=10,\n",
    "    max_simulation_results=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate QA from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# product sample data\n",
    "texts_glob = Path(\"../data/product-info/\")\n",
    "# azureai-samples data\n",
    "#texts_glob = Path(\"../../azureai-samples/scenarios/generate-synthetic-data/ai-generated-data-qna/data/data_generator_texts/\")\n",
    "files = Path.glob(texts_glob, pattern=\"**/*\")\n",
    "files = [file for file in files if Path.is_file(file)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "import nltk\n",
    "\n",
    "# download pre-trained Punkt tokenizer for sentence splitting\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "text_splitter = NLTKTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=\"cl100k_base\",  # encoding for gpt-4 and gpt-35-turbo\n",
    "    chunk_size=300,  # number of tokens to split on\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "texts = []\n",
    "for file in files:\n",
    "    loader = UnstructuredFileLoader(file)\n",
    "    docs = loader.load()\n",
    "    data = docs[0].page_content\n",
    "    texts += text_splitter.split_text(data)\n",
    "print(f\"Number of texts after splitting: {len(texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.generative.synthetic.qa import QADataGenerator, QAType\n",
    "\n",
    "## Uses AzureOpenAI environment variables\n",
    "\n",
    "# For granular logs you may set DEBUG log level:\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.DEBUG)\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "model_config = {\n",
    "    \"deployment\": \"gpt-4-1106-preview\",\n",
    "    \"model\": \"gpt-4\",\n",
    "    \"max_tokens\": 2000,\n",
    "}\n",
    "\n",
    "qa_generator = QADataGenerator(model_config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QADataGenerator(model_config=model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate QA asynchronously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.generative.synthetic.qa import QADataGenerator, QAType\n",
    "import asyncio\n",
    "from collections import Counter\n",
    "from typing import Dict\n",
    "\n",
    "concurrency = 3  # number of concurrent calls\n",
    "sem = asyncio.Semaphore(concurrency)\n",
    "\n",
    "qa_type = QAType.CONVERSATION\n",
    "\n",
    "\n",
    "async def generate_async(text: str) -> Dict:\n",
    "    async with sem:\n",
    "        return await qa_generator.generate_async(\n",
    "            text=text,\n",
    "            qa_type=qa_type,\n",
    "            num_questions=3,  # Number of questions to generate per text\n",
    "        )\n",
    "\n",
    "\n",
    "results = await asyncio.gather(*[generate_async(text) for text in texts], return_exceptions=True)\n",
    "\n",
    "question_answer_list = []\n",
    "token_usage = Counter()\n",
    "for result in results:\n",
    "    if isinstance(result, Exception):\n",
    "        raise result  # exception raised inside generate_async()\n",
    "    question_answer_list.append(result[\"question_answers\"])\n",
    "    token_usage += result[\"token_usage\"]\n",
    "\n",
    "print(\"Successfully generated QAs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Tokens used: {result['token_usage']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the generated data for later use\n",
    "Let us save the generated QnA in a format which can be understood by prompt flow (for evaluation, batch runs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "generated_dir = \"../data/generated_qa\"\n",
    "os.makedirs(generated_dir, exist_ok=True)\n",
    "output_file = os.path.join(generated_dir, \"generated_qa.jsonl\")\n",
    "qa_generator.export_to_file(output_file, qa_type, question_answer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Messing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # try using connection config\n",
    "# # from promptflow.rag.config import ConnectionConfig\n",
    "\n",
    "# model_connect_config = ConnectionConfig(\n",
    "#     subscription_id = os.environ.get(\"SUBSCRIPTION_ID\"),\n",
    "#     resource_group_name = os.environ.get(\"RESOURCE_GROUP_NAME\"),\n",
    "#     workspace_name = os.environ.get(\"PROJECT_NAME\"),\n",
    "#     connection_name = \"mssecureai4034688619\"\n",
    "\n",
    "# model_connect = AzureOpenAIModelConfiguration.from_connection(model_connect_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-evaluation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
